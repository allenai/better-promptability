{
    tasks: {
        adversarial_qa_dbert_answer_the_following_q: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "answer_the_following_q"},
        adversarial_qa_dbert_answer_the_following_q_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "answer_the_following_q_score_eval"},
        adversarial_qa_dbert_based_on: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "based_on"},
        adversarial_qa_dbert_based_on_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "based_on_score_eval"},
        adversarial_qa_dbert_generate_question: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "generate_question"},
        adversarial_qa_dbert_generate_question_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "generate_question_score_eval"},
        adversarial_qa_dbert_question_context_answer: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "question_context_answer"},
        adversarial_qa_dbert_question_context_answer_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "question_context_answer_score_eval"},
        adversarial_qa_dbert_tell_what_it_is: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "tell_what_it_is"},
        adversarial_qa_dbert_tell_what_it_is_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbert", template_name: "tell_what_it_is_score_eval"},
        adversarial_qa_dbidaf_answer_the_following_q: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "answer_the_following_q"},
        adversarial_qa_dbidaf_answer_the_following_q_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "answer_the_following_q_score_eval"},
        adversarial_qa_dbidaf_based_on: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "based_on"},
        adversarial_qa_dbidaf_based_on_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "based_on_score_eval"},
        adversarial_qa_dbidaf_generate_question: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "generate_question"},
        adversarial_qa_dbidaf_generate_question_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "generate_question_score_eval"},
        adversarial_qa_dbidaf_question_context_answer: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "question_context_answer"},
        adversarial_qa_dbidaf_question_context_answer_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "question_context_answer_score_eval"},
        adversarial_qa_dbidaf_tell_what_it_is: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "tell_what_it_is"},
        adversarial_qa_dbidaf_tell_what_it_is_score_eval: {dataset_name: "adversarial_qa", subset_name: "dbidaf", template_name: "tell_what_it_is_score_eval"},
        adversarial_qa_droberta_answer_the_following_q: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "answer_the_following_q"},
        adversarial_qa_droberta_answer_the_following_q_score_eval: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "answer_the_following_q_score_eval"},
        adversarial_qa_droberta_based_on: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "based_on"},
        adversarial_qa_droberta_based_on_score_eval: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "based_on_score_eval"},
        adversarial_qa_droberta_generate_question: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "generate_question"},
        adversarial_qa_droberta_generate_question_score_eval: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "generate_question_score_eval"},
        adversarial_qa_droberta_question_context_answer: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "question_context_answer"},
        adversarial_qa_droberta_question_context_answer_score_eval: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "question_context_answer_score_eval"},
        adversarial_qa_droberta_tell_what_it_is: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "tell_what_it_is"},
        adversarial_qa_droberta_tell_what_it_is_score_eval: {dataset_name: "adversarial_qa", subset_name: "droberta", template_name: "tell_what_it_is_score_eval"},
        ag_news_classify: {dataset_name: "ag_news", subset_name: null, template_name: "classify"},
        ag_news_classify_question_first: {dataset_name: "ag_news", subset_name: null, template_name: "classify_question_first"},
        ag_news_classify_question_first_score_eval: {dataset_name: "ag_news", subset_name: null, template_name: "classify_question_first_score_eval"},
        ag_news_classify_score_eval: {dataset_name: "ag_news", subset_name: null, template_name: "classify_score_eval"},
        ag_news_classify_with_choices: {dataset_name: "ag_news", subset_name: null, template_name: "classify_with_choices"},
        ag_news_classify_with_choices_question_first: {dataset_name: "ag_news", subset_name: null, template_name: "classify_with_choices_question_first"},
        ag_news_classify_with_choices_question_first_score_eval: {dataset_name: "ag_news", subset_name: null, template_name: "classify_with_choices_question_first_score_eval"},
        ag_news_classify_with_choices_score_eval: {dataset_name: "ag_news", subset_name: null, template_name: "classify_with_choices_score_eval"},
        ag_news_recommend: {dataset_name: "ag_news", subset_name: null, template_name: "recommend"},
        ag_news_recommend_score_eval: {dataset_name: "ag_news", subset_name: null, template_name: "recommend_score_eval"},
        ag_news_which_section: {dataset_name: "ag_news", subset_name: null, template_name: "which_section"},
        ag_news_which_section_choices: {dataset_name: "ag_news", subset_name: null, template_name: "which_section_choices"},
        ag_news_which_section_choices_score_eval: {dataset_name: "ag_news", subset_name: null, template_name: "which_section_choices_score_eval"},
        ag_news_which_section_score_eval: {dataset_name: "ag_news", subset_name: null, template_name: "which_section_score_eval"},
        ai2_arc_ARC_Challenge_heres_a_problem: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "heres_a_problem"},
        ai2_arc_ARC_Challenge_heres_a_problem_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "heres_a_problem_score_eval"},
        ai2_arc_ARC_Challenge_i_am_hesitating: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "i_am_hesitating"},
        ai2_arc_ARC_Challenge_i_am_hesitating_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "i_am_hesitating_score_eval"},
        ai2_arc_ARC_Challenge_multiple_choice: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "multiple_choice"},
        ai2_arc_ARC_Challenge_multiple_choice_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "multiple_choice_score_eval"},
        ai2_arc_ARC_Challenge_pick_false_options: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "pick_false_options"},
        ai2_arc_ARC_Challenge_pick_false_options_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "pick_false_options_score_eval"},
        ai2_arc_ARC_Challenge_pick_the_most_correct_option: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "pick_the_most_correct_option"},
        ai2_arc_ARC_Challenge_pick_the_most_correct_option_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "pick_the_most_correct_option_score_eval"},
        ai2_arc_ARC_Challenge_qa_options: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "qa_options"},
        ai2_arc_ARC_Challenge_qa_options_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Challenge", template_name: "qa_options_score_eval"},
        ai2_arc_ARC_Easy_heres_a_problem: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "heres_a_problem"},
        ai2_arc_ARC_Easy_heres_a_problem_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "heres_a_problem_score_eval"},
        ai2_arc_ARC_Easy_i_am_hesitating: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "i_am_hesitating"},
        ai2_arc_ARC_Easy_i_am_hesitating_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "i_am_hesitating_score_eval"},
        ai2_arc_ARC_Easy_multiple_choice: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "multiple_choice"},
        ai2_arc_ARC_Easy_multiple_choice_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "multiple_choice_score_eval"},
        ai2_arc_ARC_Easy_pick_false_options: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "pick_false_options"},
        ai2_arc_ARC_Easy_pick_false_options_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "pick_false_options_score_eval"},
        ai2_arc_ARC_Easy_pick_the_most_correct_option: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "pick_the_most_correct_option"},
        ai2_arc_ARC_Easy_pick_the_most_correct_option_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "pick_the_most_correct_option_score_eval"},
        ai2_arc_ARC_Easy_qa_options: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "qa_options"},
        ai2_arc_ARC_Easy_qa_options_score_eval: {dataset_name: "ai2_arc", subset_name: "ARC-Easy", template_name: "qa_options_score_eval"},
        amazon_polarity_Is_this_product_review_positive: {dataset_name: "amazon_polarity", subset_name: null, template_name: "Is_this_product_review_positive"},
        amazon_polarity_Is_this_product_review_positive_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "Is_this_product_review_positive_score_eval"},
        amazon_polarity_Is_this_review: {dataset_name: "amazon_polarity", subset_name: null, template_name: "Is_this_review"},
        amazon_polarity_Is_this_review_negative: {dataset_name: "amazon_polarity", subset_name: null, template_name: "Is_this_review_negative"},
        amazon_polarity_Is_this_review_negative_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "Is_this_review_negative_score_eval"},
        amazon_polarity_Is_this_review_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "Is_this_review_score_eval"},
        amazon_polarity_User_recommend_this_product: {dataset_name: "amazon_polarity", subset_name: null, template_name: "User_recommend_this_product"},
        amazon_polarity_User_recommend_this_product_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "User_recommend_this_product_score_eval"},
        amazon_polarity_convey_negative_or_positive_sentiment: {dataset_name: "amazon_polarity", subset_name: null, template_name: "convey_negative_or_positive_sentiment"},
        amazon_polarity_convey_negative_or_positive_sentiment_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "convey_negative_or_positive_sentiment_score_eval"},
        amazon_polarity_flattering_or_not: {dataset_name: "amazon_polarity", subset_name: null, template_name: "flattering_or_not"},
        amazon_polarity_flattering_or_not_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "flattering_or_not_score_eval"},
        amazon_polarity_negative_or_positive_tone: {dataset_name: "amazon_polarity", subset_name: null, template_name: "negative_or_positive_tone"},
        amazon_polarity_negative_or_positive_tone_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "negative_or_positive_tone_score_eval"},
        amazon_polarity_user_satisfied: {dataset_name: "amazon_polarity", subset_name: null, template_name: "user_satisfied"},
        amazon_polarity_user_satisfied_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "user_satisfied_score_eval"},
        amazon_polarity_would_you_buy: {dataset_name: "amazon_polarity", subset_name: null, template_name: "would_you_buy"},
        amazon_polarity_would_you_buy_score_eval: {dataset_name: "amazon_polarity", subset_name: null, template_name: "would_you_buy_score_eval"},
        anli_GPT_3_style_r1: {dataset_name: "anli", subset_name: "r1", template_name: "GPT-3 style"},
        anli_GPT_3_style_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "GPT-3 style_score_eval"},
        anli_GPT_3_style_r2: {dataset_name: "anli", subset_name: "r2", template_name: "GPT-3 style"},
        anli_GPT_3_style_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "GPT-3 style_score_eval"},
        anli_GPT_3_style_r3: {dataset_name: "anli", subset_name: "r3", template_name: "GPT-3 style"},
        anli_GPT_3_style_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "GPT-3 style_score_eval"},
        anli_MNLI_crowdsource_r1: {dataset_name: "anli", subset_name: "r1", template_name: "MNLI crowdsource"},
        anli_MNLI_crowdsource_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "MNLI crowdsource_score_eval"},
        anli_MNLI_crowdsource_r2: {dataset_name: "anli", subset_name: "r2", template_name: "MNLI crowdsource"},
        anli_MNLI_crowdsource_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "MNLI crowdsource_score_eval"},
        anli_MNLI_crowdsource_r3: {dataset_name: "anli", subset_name: "r3", template_name: "MNLI crowdsource"},
        anli_MNLI_crowdsource_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "MNLI crowdsource_score_eval"},
        anli_always_sometimes_never_r1: {dataset_name: "anli", subset_name: "r1", template_name: "always/sometimes/never"},
        anli_always_sometimes_never_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "always/sometimes/never_score_eval"},
        anli_always_sometimes_never_r2: {dataset_name: "anli", subset_name: "r2", template_name: "always/sometimes/never"},
        anli_always_sometimes_never_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "always/sometimes/never_score_eval"},
        anli_always_sometimes_never_r3: {dataset_name: "anli", subset_name: "r3", template_name: "always/sometimes/never"},
        anli_always_sometimes_never_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "always/sometimes/never_score_eval"},
        anli_based_on_the_previous_passage_r1: {dataset_name: "anli", subset_name: "r1", template_name: "based on the previous passage"},
        anli_based_on_the_previous_passage_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "based on the previous passage_score_eval"},
        anli_based_on_the_previous_passage_r2: {dataset_name: "anli", subset_name: "r2", template_name: "based on the previous passage"},
        anli_based_on_the_previous_passage_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "based on the previous passage_score_eval"},
        anli_based_on_the_previous_passage_r3: {dataset_name: "anli", subset_name: "r3", template_name: "based on the previous passage"},
        anli_based_on_the_previous_passage_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "based on the previous passage_score_eval"},
        anli_can_we_infer_r1: {dataset_name: "anli", subset_name: "r1", template_name: "can we infer"},
        anli_can_we_infer_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "can we infer_score_eval"},
        anli_can_we_infer_r2: {dataset_name: "anli", subset_name: "r2", template_name: "can we infer"},
        anli_can_we_infer_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "can we infer_score_eval"},
        anli_can_we_infer_r3: {dataset_name: "anli", subset_name: "r3", template_name: "can we infer"},
        anli_can_we_infer_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "can we infer_score_eval"},
        anli_claim_true_false_inconclusive_r1: {dataset_name: "anli", subset_name: "r1", template_name: "claim true/false/inconclusive"},
        anli_claim_true_false_inconclusive_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "claim true/false/inconclusive_score_eval"},
        anli_claim_true_false_inconclusive_r2: {dataset_name: "anli", subset_name: "r2", template_name: "claim true/false/inconclusive"},
        anli_claim_true_false_inconclusive_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "claim true/false/inconclusive_score_eval"},
        anli_claim_true_false_inconclusive_r3: {dataset_name: "anli", subset_name: "r3", template_name: "claim true/false/inconclusive"},
        anli_claim_true_false_inconclusive_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "claim true/false/inconclusive_score_eval"},
        anli_consider_always_sometimes_never_r1: {dataset_name: "anli", subset_name: "r1", template_name: "consider always/sometimes/never"},
        anli_consider_always_sometimes_never_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "consider always/sometimes/never_score_eval"},
        anli_consider_always_sometimes_never_r2: {dataset_name: "anli", subset_name: "r2", template_name: "consider always/sometimes/never"},
        anli_consider_always_sometimes_never_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "consider always/sometimes/never_score_eval"},
        anli_consider_always_sometimes_never_r3: {dataset_name: "anli", subset_name: "r3", template_name: "consider always/sometimes/never"},
        anli_consider_always_sometimes_never_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "consider always/sometimes/never_score_eval"},
        anli_does_it_follow_that_r1: {dataset_name: "anli", subset_name: "r1", template_name: "does it follow that"},
        anli_does_it_follow_that_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "does it follow that_score_eval"},
        anli_does_it_follow_that_r2: {dataset_name: "anli", subset_name: "r2", template_name: "does it follow that"},
        anli_does_it_follow_that_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "does it follow that_score_eval"},
        anli_does_it_follow_that_r3: {dataset_name: "anli", subset_name: "r3", template_name: "does it follow that"},
        anli_does_it_follow_that_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "does it follow that_score_eval"},
        anli_does_this_imply_r1: {dataset_name: "anli", subset_name: "r1", template_name: "does this imply"},
        anli_does_this_imply_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "does this imply_score_eval"},
        anli_does_this_imply_r2: {dataset_name: "anli", subset_name: "r2", template_name: "does this imply"},
        anli_does_this_imply_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "does this imply_score_eval"},
        anli_does_this_imply_r3: {dataset_name: "anli", subset_name: "r3", template_name: "does this imply"},
        anli_does_this_imply_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "does this imply_score_eval"},
        anli_guaranteed_possible_impossible_r1: {dataset_name: "anli", subset_name: "r1", template_name: "guaranteed/possible/impossible"},
        anli_guaranteed_possible_impossible_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "guaranteed/possible/impossible_score_eval"},
        anli_guaranteed_possible_impossible_r2: {dataset_name: "anli", subset_name: "r2", template_name: "guaranteed/possible/impossible"},
        anli_guaranteed_possible_impossible_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "guaranteed/possible/impossible_score_eval"},
        anli_guaranteed_possible_impossible_r3: {dataset_name: "anli", subset_name: "r3", template_name: "guaranteed/possible/impossible"},
        anli_guaranteed_possible_impossible_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "guaranteed/possible/impossible_score_eval"},
        anli_guaranteed_true_r1: {dataset_name: "anli", subset_name: "r1", template_name: "guaranteed true"},
        anli_guaranteed_true_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "guaranteed true_score_eval"},
        anli_guaranteed_true_r2: {dataset_name: "anli", subset_name: "r2", template_name: "guaranteed true"},
        anli_guaranteed_true_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "guaranteed true_score_eval"},
        anli_guaranteed_true_r3: {dataset_name: "anli", subset_name: "r3", template_name: "guaranteed true"},
        anli_guaranteed_true_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "guaranteed true_score_eval"},
        anli_justified_in_saying_r1: {dataset_name: "anli", subset_name: "r1", template_name: "justified in saying"},
        anli_justified_in_saying_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "justified in saying_score_eval"},
        anli_justified_in_saying_r2: {dataset_name: "anli", subset_name: "r2", template_name: "justified in saying"},
        anli_justified_in_saying_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "justified in saying_score_eval"},
        anli_justified_in_saying_r3: {dataset_name: "anli", subset_name: "r3", template_name: "justified in saying"},
        anli_justified_in_saying_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "justified in saying_score_eval"},
        anli_must_be_true_r1: {dataset_name: "anli", subset_name: "r1", template_name: "must be true"},
        anli_must_be_true_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "must be true_score_eval"},
        anli_must_be_true_r2: {dataset_name: "anli", subset_name: "r2", template_name: "must be true"},
        anli_must_be_true_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "must be true_score_eval"},
        anli_must_be_true_r3: {dataset_name: "anli", subset_name: "r3", template_name: "must be true"},
        anli_must_be_true_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "must be true_score_eval"},
        anli_should_assume_r1: {dataset_name: "anli", subset_name: "r1", template_name: "should assume"},
        anli_should_assume_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "should assume_score_eval"},
        anli_should_assume_r2: {dataset_name: "anli", subset_name: "r2", template_name: "should assume"},
        anli_should_assume_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "should assume_score_eval"},
        anli_should_assume_r3: {dataset_name: "anli", subset_name: "r3", template_name: "should assume"},
        anli_should_assume_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "should assume_score_eval"},
        anli_take_the_following_as_truth_r1: {dataset_name: "anli", subset_name: "r1", template_name: "take the following as truth"},
        anli_take_the_following_as_truth_r1_score_eval: {dataset_name: "anli", subset_name: "r1", template_name: "take the following as truth_score_eval"},
        anli_take_the_following_as_truth_r2: {dataset_name: "anli", subset_name: "r2", template_name: "take the following as truth"},
        anli_take_the_following_as_truth_r2_score_eval: {dataset_name: "anli", subset_name: "r2", template_name: "take the following as truth_score_eval"},
        anli_take_the_following_as_truth_r3: {dataset_name: "anli", subset_name: "r3", template_name: "take the following as truth"},
        anli_take_the_following_as_truth_r3_score_eval: {dataset_name: "anli", subset_name: "r3", template_name: "take the following as truth_score_eval"},
        app_reviews_categorize_rating_using_review: {dataset_name: "app_reviews", subset_name: null, template_name: "categorize_rating_using_review"},
        app_reviews_categorize_rating_using_review_score_eval: {dataset_name: "app_reviews", subset_name: null, template_name: "categorize_rating_using_review_score_eval"},
        app_reviews_convert_to_rating: {dataset_name: "app_reviews", subset_name: null, template_name: "convert_to_rating"},
        app_reviews_convert_to_rating_score_eval: {dataset_name: "app_reviews", subset_name: null, template_name: "convert_to_rating_score_eval"},
        app_reviews_convert_to_star_rating: {dataset_name: "app_reviews", subset_name: null, template_name: "convert_to_star_rating"},
        app_reviews_convert_to_star_rating_score_eval: {dataset_name: "app_reviews", subset_name: null, template_name: "convert_to_star_rating_score_eval"},
        app_reviews_generate_review: {dataset_name: "app_reviews", subset_name: null, template_name: "generate_review"},
        app_reviews_generate_review_score_eval: {dataset_name: "app_reviews", subset_name: null, template_name: "generate_review_score_eval"},
        circa_goldstandard1_judgement: {dataset_name: "circa", subset_name: null, template_name: "goldstandard1_judgement"},
        circa_goldstandard1_judgement_score_eval: {dataset_name: "circa", subset_name: null, template_name: "goldstandard1_judgement_score_eval"},
        circa_goldstandard2_judgement: {dataset_name: "circa", subset_name: null, template_name: "goldstandard2_judgement"},
        circa_goldstandard2_judgement_score_eval: {dataset_name: "circa", subset_name: null, template_name: "goldstandard2_judgement_score_eval"},
        circa_judgement: {dataset_name: "circa", subset_name: null, template_name: "judgement"},
        circa_judgement_score_eval: {dataset_name: "circa", subset_name: null, template_name: "judgement_score_eval"},
        circa_possible_qn: {dataset_name: "circa", subset_name: null, template_name: "possible_qn"},
        circa_possible_qn_score_eval: {dataset_name: "circa", subset_name: null, template_name: "possible_qn_score_eval"},
        circa_question_declarative: {dataset_name: "circa", subset_name: null, template_name: "question_declarative"},
        circa_question_declarative_score_eval: {dataset_name: "circa", subset_name: null, template_name: "question_declarative_score_eval"},
        "cnn_dailymail_3.0.0_2_or_3_sentences": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "2_or_3_sentences"},
        "cnn_dailymail_3.0.0_2_or_3_sentences_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "2_or_3_sentences_score_eval"},
        "cnn_dailymail_3.0.0_generate_story": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "generate_story"},
        "cnn_dailymail_3.0.0_generate_story_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "generate_story_score_eval"},
        "cnn_dailymail_3.0.0_news_card_view": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "news_card_view"},
        "cnn_dailymail_3.0.0_news_card_view_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "news_card_view_score_eval"},
        "cnn_dailymail_3.0.0_news_stock": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "news_stock"},
        "cnn_dailymail_3.0.0_news_stock_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "news_stock_score_eval"},
        "cnn_dailymail_3.0.0_news_summary": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "news_summary"},
        "cnn_dailymail_3.0.0_news_summary_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "news_summary_score_eval"},
        "cnn_dailymail_3.0.0_spice_up_story": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "spice_up_story"},
        "cnn_dailymail_3.0.0_spice_up_story_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "spice_up_story_score_eval"},
        "cnn_dailymail_3.0.0_sum_in_brief": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "sum_in_brief"},
        "cnn_dailymail_3.0.0_sum_in_brief_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "sum_in_brief_score_eval"},
        "cnn_dailymail_3.0.0_tldr_summary": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "tldr_summary"},
        "cnn_dailymail_3.0.0_tldr_summary_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "tldr_summary_score_eval"},
        "cnn_dailymail_3.0.0_write_an_outline": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "write_an_outline"},
        "cnn_dailymail_3.0.0_write_an_outline_score_eval": {dataset_name: "cnn_dailymail", subset_name: "3.0.0", template_name: "write_an_outline_score_eval"},
        common_gen_Example_prompt: {dataset_name: "common_gen", subset_name: null, template_name: "Example prompt"},
        common_gen_Example_prompt_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "Example prompt_score_eval"},
        common_gen_Given_concepts_type_1: {dataset_name: "common_gen", subset_name: null, template_name: "Given concepts type 1"},
        common_gen_Given_concepts_type_1_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "Given concepts type 1_score_eval"},
        common_gen_Given_concepts_type_2: {dataset_name: "common_gen", subset_name: null, template_name: "Given concepts - type 2"},
        common_gen_Given_concepts_type_2_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "Given concepts - type 2_score_eval"},
        common_gen_Put_together: {dataset_name: "common_gen", subset_name: null, template_name: "Put together"},
        common_gen_Put_together_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "Put together_score_eval"},
        common_gen_choice_in_concept_centric_sentence_generation: {dataset_name: "common_gen", subset_name: null, template_name: "choice in concept centric sentence generation"},
        common_gen_choice_in_concept_centric_sentence_generation_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "choice in concept centric sentence generation_score_eval"},
        common_gen_random_task_template_prompt: {dataset_name: "common_gen", subset_name: null, template_name: "random task template prompt"},
        common_gen_random_task_template_prompt_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "random task template prompt_score_eval"},
        common_gen_sentence_to_concepts: {dataset_name: "common_gen", subset_name: null, template_name: "sentence to concepts"},
        common_gen_sentence_to_concepts_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "sentence to concepts_score_eval"},
        common_gen_topic_to_sentence: {dataset_name: "common_gen", subset_name: null, template_name: "topic to sentence"},
        common_gen_topic_to_sentence_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "topic to sentence_score_eval"},
        common_gen_topics_from_the_sentence: {dataset_name: "common_gen", subset_name: null, template_name: "topics from the sentence"},
        common_gen_topics_from_the_sentence_score_eval: {dataset_name: "common_gen", subset_name: null, template_name: "topics from the sentence_score_eval"},
        coqa_extract_answer_first_qa_turn: {dataset_name: "coqa", subset_name: null, template_name: "extract_answer_first_qa_turn"},
        coqa_extract_answer_first_qa_turn_score_eval: {dataset_name: "coqa", subset_name: null, template_name: "extract_answer_first_qa_turn_score_eval"},
        coqa_first_qa_turn: {dataset_name: "coqa", subset_name: null, template_name: "first_qa_turn"},
        coqa_first_qa_turn_score_eval: {dataset_name: "coqa", subset_name: null, template_name: "first_qa_turn_score_eval"},
        coqa_generate_dialogue: {dataset_name: "coqa", subset_name: null, template_name: "generate_dialogue"},
        coqa_generate_dialogue_score_eval: {dataset_name: "coqa", subset_name: null, template_name: "generate_dialogue_score_eval"},
        coqa_last_qa_turn: {dataset_name: "coqa", subset_name: null, template_name: "last_qa_turn"},
        coqa_last_qa_turn_score_eval: {dataset_name: "coqa", subset_name: null, template_name: "last_qa_turn_score_eval"},
        coqa_missing_answer: {dataset_name: "coqa", subset_name: null, template_name: "missing_answer"},
        coqa_missing_answer_score_eval: {dataset_name: "coqa", subset_name: null, template_name: "missing_answer_score_eval"},
        "cos_e_v1.11_aligned_with_common_sense": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "aligned_with_common_sense"},
        "cos_e_v1.11_aligned_with_common_sense_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "aligned_with_common_sense_score_eval"},
        "cos_e_v1.11_description_question_option_id": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "description_question_option_id"},
        "cos_e_v1.11_description_question_option_id_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "description_question_option_id_score_eval"},
        "cos_e_v1.11_description_question_option_text": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "description_question_option_text"},
        "cos_e_v1.11_description_question_option_text_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "description_question_option_text_score_eval"},
        "cos_e_v1.11_explain_why_human": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "explain_why_human"},
        "cos_e_v1.11_explain_why_human_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "explain_why_human_score_eval"},
        "cos_e_v1.11_generate_explanation_given_text": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "generate_explanation_given_text"},
        "cos_e_v1.11_generate_explanation_given_text_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "generate_explanation_given_text_score_eval"},
        "cos_e_v1.11_i_think": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "i_think"},
        "cos_e_v1.11_i_think_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "i_think_score_eval"},
        "cos_e_v1.11_question_description_option_id": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_description_option_id"},
        "cos_e_v1.11_question_description_option_id_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_description_option_id_score_eval"},
        "cos_e_v1.11_question_description_option_text": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_description_option_text"},
        "cos_e_v1.11_question_description_option_text_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_description_option_text_score_eval"},
        "cos_e_v1.11_question_option_description_id": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_option_description_id"},
        "cos_e_v1.11_question_option_description_id_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_option_description_id_score_eval"},
        "cos_e_v1.11_question_option_description_text": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_option_description_text"},
        "cos_e_v1.11_question_option_description_text_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "question_option_description_text_score_eval"},
        "cos_e_v1.11_rationale": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "rationale"},
        "cos_e_v1.11_rationale_score_eval": {dataset_name: "cos_e", subset_name: "v1.11", template_name: "rationale_score_eval"},
        cosmos_qa_context_answer_to_question: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_answer_to_question"},
        cosmos_qa_context_answer_to_question_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_answer_to_question_score_eval"},
        cosmos_qa_context_description_question_answer_id: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_description_question_answer_id"},
        cosmos_qa_context_description_question_answer_id_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_description_question_answer_id_score_eval"},
        cosmos_qa_context_description_question_answer_text: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_description_question_answer_text"},
        cosmos_qa_context_description_question_answer_text_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_description_question_answer_text_score_eval"},
        cosmos_qa_context_description_question_text: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_description_question_text"},
        cosmos_qa_context_description_question_text_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_description_question_text_score_eval"},
        cosmos_qa_context_question_description_answer_id: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_question_description_answer_id"},
        cosmos_qa_context_question_description_answer_id_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_question_description_answer_id_score_eval"},
        cosmos_qa_context_question_description_answer_text: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_question_description_answer_text"},
        cosmos_qa_context_question_description_answer_text_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_question_description_answer_text_score_eval"},
        cosmos_qa_context_question_description_text: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_question_description_text"},
        cosmos_qa_context_question_description_text_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "context_question_description_text_score_eval"},
        cosmos_qa_description_context_question_answer_id: {dataset_name: "cosmos_qa", subset_name: null, template_name: "description_context_question_answer_id"},
        cosmos_qa_description_context_question_answer_id_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "description_context_question_answer_id_score_eval"},
        cosmos_qa_description_context_question_answer_text: {dataset_name: "cosmos_qa", subset_name: null, template_name: "description_context_question_answer_text"},
        cosmos_qa_description_context_question_answer_text_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "description_context_question_answer_text_score_eval"},
        cosmos_qa_description_context_question_text: {dataset_name: "cosmos_qa", subset_name: null, template_name: "description_context_question_text"},
        cosmos_qa_description_context_question_text_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "description_context_question_text_score_eval"},
        cosmos_qa_no_prompt_id: {dataset_name: "cosmos_qa", subset_name: null, template_name: "no_prompt_id"},
        cosmos_qa_no_prompt_id_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "no_prompt_id_score_eval"},
        cosmos_qa_no_prompt_text: {dataset_name: "cosmos_qa", subset_name: null, template_name: "no_prompt_text"},
        cosmos_qa_no_prompt_text_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "no_prompt_text_score_eval"},
        cosmos_qa_only_question_answer: {dataset_name: "cosmos_qa", subset_name: null, template_name: "only_question_answer"},
        cosmos_qa_only_question_answer_score_eval: {dataset_name: "cosmos_qa", subset_name: null, template_name: "only_question_answer_score_eval"},
        craffel_openai_lambada_GPT_3_style: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "GPT-3 style"},
        craffel_openai_lambada_GPT_3_style_score_eval: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "GPT-3 style_score_eval"},
        craffel_openai_lambada_ellipses: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "ellipses"},
        craffel_openai_lambada_ellipses_score_eval: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "ellipses_score_eval"},
        craffel_openai_lambada_fill_in_the_____: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "fill in the ____"},
        craffel_openai_lambada_fill_in_the______score_eval: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "fill in the _____score_eval"},
        craffel_openai_lambada_please_next_word: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "please next word"},
        craffel_openai_lambada_please_next_word_score_eval: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "please next word_score_eval"},
        craffel_openai_lambada_what_comes_next: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "what comes next"},
        craffel_openai_lambada_what_comes_next_score_eval: {dataset_name: "craffel/openai_lambada", subset_name: null, template_name: "what comes next_score_eval"},
        crows_pairs_anti_stereotype: {dataset_name: "crows_pairs", subset_name: null, template_name: "anti_stereotype"},
        crows_pairs_anti_stereotype_confirm: {dataset_name: "crows_pairs", subset_name: null, template_name: "anti_stereotype_confirm"},
        crows_pairs_anti_stereotype_confirm_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "anti_stereotype_confirm_score_eval"},
        crows_pairs_anti_stereotype_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "anti_stereotype_score_eval"},
        crows_pairs_demontraste_or_violate: {dataset_name: "crows_pairs", subset_name: null, template_name: "demontraste_or_violate"},
        crows_pairs_demontraste_or_violate_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "demontraste_or_violate_score_eval"},
        crows_pairs_stereotype: {dataset_name: "crows_pairs", subset_name: null, template_name: "stereotype"},
        crows_pairs_stereotype_confirm: {dataset_name: "crows_pairs", subset_name: null, template_name: "stereotype_confirm"},
        crows_pairs_stereotype_confirm_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "stereotype_confirm_score_eval"},
        crows_pairs_stereotype_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "stereotype_score_eval"},
        crows_pairs_transform_anti_stereo: {dataset_name: "crows_pairs", subset_name: null, template_name: "transform_anti_stereo"},
        crows_pairs_transform_anti_stereo_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "transform_anti_stereo_score_eval"},
        crows_pairs_transform_stereo: {dataset_name: "crows_pairs", subset_name: null, template_name: "transform_stereo"},
        crows_pairs_transform_stereo_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "transform_stereo_score_eval"},
        crows_pairs_which_bias: {dataset_name: "crows_pairs", subset_name: null, template_name: "which_bias"},
        crows_pairs_which_bias_score_eval: {dataset_name: "crows_pairs", subset_name: null, template_name: "which_bias_score_eval"},
        dbpedia_14_given_a_choice_of_categories_: {dataset_name: "dbpedia_14", subset_name: null, template_name: "given_a_choice_of_categories "},
        dbpedia_14_given_a_choice_of_categories__score_eval: {dataset_name: "dbpedia_14", subset_name: null, template_name: "given_a_choice_of_categories _score_eval"},
        dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to: {dataset_name: "dbpedia_14", subset_name: null, template_name: "given_a_list_of_category_what_does_the_title_belong_to"},
        dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to_score_eval: {dataset_name: "dbpedia_14", subset_name: null, template_name: "given_a_list_of_category_what_does_the_title_belong_to_score_eval"},
        dbpedia_14_given_list_what_category_does_the_paragraph_belong_to: {dataset_name: "dbpedia_14", subset_name: null, template_name: "given_list_what_category_does_the_paragraph_belong_to"},
        dbpedia_14_given_list_what_category_does_the_paragraph_belong_to_score_eval: {dataset_name: "dbpedia_14", subset_name: null, template_name: "given_list_what_category_does_the_paragraph_belong_to_score_eval"},
        dbpedia_14_pick_one_category_for_the_following_text: {dataset_name: "dbpedia_14", subset_name: null, template_name: "pick_one_category_for_the_following_text"},
        dbpedia_14_pick_one_category_for_the_following_text_score_eval: {dataset_name: "dbpedia_14", subset_name: null, template_name: "pick_one_category_for_the_following_text_score_eval"},
        dream_answer_to_dialogue: {dataset_name: "dream", subset_name: null, template_name: "answer-to-dialogue"},
        dream_answer_to_dialogue_score_eval: {dataset_name: "dream", subset_name: null, template_name: "answer-to-dialogue_score_eval"},
        dream_baseline: {dataset_name: "dream", subset_name: null, template_name: "baseline"},
        dream_baseline_score_eval: {dataset_name: "dream", subset_name: null, template_name: "baseline_score_eval"},
        dream_generate_first_utterance: {dataset_name: "dream", subset_name: null, template_name: "generate-first-utterance"},
        dream_generate_first_utterance_score_eval: {dataset_name: "dream", subset_name: null, template_name: "generate-first-utterance_score_eval"},
        dream_generate_last_utterance: {dataset_name: "dream", subset_name: null, template_name: "generate-last-utterance"},
        dream_generate_last_utterance_score_eval: {dataset_name: "dream", subset_name: null, template_name: "generate-last-utterance_score_eval"},
        dream_read_the_following_conversation_and_answer_the_question: {dataset_name: "dream", subset_name: null, template_name: "read_the_following_conversation_and_answer_the_question"},
        dream_read_the_following_conversation_and_answer_the_question_score_eval: {dataset_name: "dream", subset_name: null, template_name: "read_the_following_conversation_and_answer_the_question_score_eval"},
        drop_DROP_GPT3: {dataset_name: "drop", subset_name: null, template_name: "DROP GPT3"},
        drop_DROP_GPT3_score_eval: {dataset_name: "drop", subset_name: null, template_name: "DROP GPT3_score_eval"},
        drop_can_you_tell_me: {dataset_name: "drop", subset_name: null, template_name: "can you tell me"},
        drop_can_you_tell_me_score_eval: {dataset_name: "drop", subset_name: null, template_name: "can you tell me_score_eval"},
        drop_context_question_answer: {dataset_name: "drop", subset_name: null, template_name: "context question answer"},
        drop_context_question_answer_score_eval: {dataset_name: "drop", subset_name: null, template_name: "context question answer_score_eval"},
        drop_generate_question_with_passage_and_answer: {dataset_name: "drop", subset_name: null, template_name: "generate_question_with_passage_and_answer"},
        drop_generate_question_with_passage_and_answer_score_eval: {dataset_name: "drop", subset_name: null, template_name: "generate_question_with_passage_and_answer_score_eval"},
        drop_question_context_answer: {dataset_name: "drop", subset_name: null, template_name: "question context answer"},
        drop_question_context_answer_score_eval: {dataset_name: "drop", subset_name: null, template_name: "question context answer_score_eval"},
        duorc_ParaphraseRC_answer_question: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "answer_question"},
        duorc_ParaphraseRC_answer_question_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "answer_question_score_eval"},
        duorc_ParaphraseRC_build_story_around_qa: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "build_story_around_qa"},
        duorc_ParaphraseRC_build_story_around_qa_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "build_story_around_qa_score_eval"},
        duorc_ParaphraseRC_decide_worth_it: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "decide_worth_it"},
        duorc_ParaphraseRC_decide_worth_it_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "decide_worth_it_score_eval"},
        duorc_ParaphraseRC_extract_answer: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "extract_answer"},
        duorc_ParaphraseRC_extract_answer_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "extract_answer_score_eval"},
        duorc_ParaphraseRC_generate_question: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "generate_question"},
        duorc_ParaphraseRC_generate_question_by_answer: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "generate_question_by_answer"},
        duorc_ParaphraseRC_generate_question_by_answer_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "generate_question_by_answer_score_eval"},
        duorc_ParaphraseRC_generate_question_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "generate_question_score_eval"},
        duorc_ParaphraseRC_movie_director: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "movie_director"},
        duorc_ParaphraseRC_movie_director_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "movie_director_score_eval"},
        duorc_ParaphraseRC_question_answering: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "question_answering"},
        duorc_ParaphraseRC_question_answering_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "question_answering_score_eval"},
        duorc_ParaphraseRC_title_generation: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "title_generation"},
        duorc_ParaphraseRC_title_generation_score_eval: {dataset_name: "duorc", subset_name: "ParaphraseRC", template_name: "title_generation_score_eval"},
        duorc_SelfRC_answer_question: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "answer_question"},
        duorc_SelfRC_answer_question_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "answer_question_score_eval"},
        duorc_SelfRC_build_story_around_qa: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "build_story_around_qa"},
        duorc_SelfRC_build_story_around_qa_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "build_story_around_qa_score_eval"},
        duorc_SelfRC_decide_worth_it: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "decide_worth_it"},
        duorc_SelfRC_decide_worth_it_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "decide_worth_it_score_eval"},
        duorc_SelfRC_extract_answer: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "extract_answer"},
        duorc_SelfRC_extract_answer_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "extract_answer_score_eval"},
        duorc_SelfRC_generate_question: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "generate_question"},
        duorc_SelfRC_generate_question_by_answer: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "generate_question_by_answer"},
        duorc_SelfRC_generate_question_by_answer_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "generate_question_by_answer_score_eval"},
        duorc_SelfRC_generate_question_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "generate_question_score_eval"},
        duorc_SelfRC_movie_director: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "movie_director"},
        duorc_SelfRC_movie_director_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "movie_director_score_eval"},
        duorc_SelfRC_question_answering: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "question_answering"},
        duorc_SelfRC_question_answering_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "question_answering_score_eval"},
        duorc_SelfRC_title_generation: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "title_generation"},
        duorc_SelfRC_title_generation_score_eval: {dataset_name: "duorc", subset_name: "SelfRC", template_name: "title_generation_score_eval"},
        gigaword_TLDR: {dataset_name: "gigaword", subset_name: null, template_name: "TLDR"},
        gigaword_TLDR_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "TLDR_score_eval"},
        gigaword_first_sentence_title: {dataset_name: "gigaword", subset_name: null, template_name: "first_sentence_title"},
        gigaword_first_sentence_title_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "first_sentence_title_score_eval"},
        gigaword_generate_summary_for_this: {dataset_name: "gigaword", subset_name: null, template_name: "generate_summary_for_this"},
        gigaword_generate_summary_for_this_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "generate_summary_for_this_score_eval"},
        gigaword_in_a_nutshell: {dataset_name: "gigaword", subset_name: null, template_name: "in_a_nutshell"},
        gigaword_in_a_nutshell_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "in_a_nutshell_score_eval"},
        gigaword_make_a_title: {dataset_name: "gigaword", subset_name: null, template_name: "make_a_title"},
        gigaword_make_a_title_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "make_a_title_score_eval"},
        gigaword_reverse_writing: {dataset_name: "gigaword", subset_name: null, template_name: "reverse_writing"},
        gigaword_reverse_writing_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "reverse_writing_score_eval"},
        gigaword_write_a_title_for_this_sentence: {dataset_name: "gigaword", subset_name: null, template_name: "write_a_title_for_this_sentence"},
        gigaword_write_a_title_for_this_sentence_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "write_a_title_for_this_sentence_score_eval"},
        gigaword_write_an_article: {dataset_name: "gigaword", subset_name: null, template_name: "write_an_article"},
        gigaword_write_an_article_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "write_an_article_score_eval"},
        gigaword_write_its_sentence: {dataset_name: "gigaword", subset_name: null, template_name: "write_its_sentence"},
        gigaword_write_its_sentence_score_eval: {dataset_name: "gigaword", subset_name: null, template_name: "write_its_sentence_score_eval"},
        glue_cola_Following_sentence_acceptable: {dataset_name: "glue", subset_name: "cola", template_name: "Following sentence acceptable"},
        glue_cola_Following_sentence_acceptable_score_eval: {dataset_name: "glue", subset_name: "cola", template_name: "Following sentence acceptable_score_eval"},
        glue_cola_Make_sense_yes_no: {dataset_name: "glue", subset_name: "cola", template_name: "Make sense yes no"},
        glue_cola_Make_sense_yes_no_score_eval: {dataset_name: "glue", subset_name: "cola", template_name: "Make sense yes no_score_eval"},
        glue_cola_Previous_sentence_acceptable: {dataset_name: "glue", subset_name: "cola", template_name: "Previous sentence acceptable"},
        glue_cola_Previous_sentence_acceptable_score_eval: {dataset_name: "glue", subset_name: "cola", template_name: "Previous sentence acceptable_score_eval"},
        glue_cola_editing: {dataset_name: "glue", subset_name: "cola", template_name: "editing"},
        glue_cola_editing_score_eval: {dataset_name: "glue", subset_name: "cola", template_name: "editing_score_eval"},
        glue_cola_is_this_correct: {dataset_name: "glue", subset_name: "cola", template_name: "is_this_correct"},
        glue_cola_is_this_correct_score_eval: {dataset_name: "glue", subset_name: "cola", template_name: "is_this_correct_score_eval"},
        glue_mrpc_equivalent: {dataset_name: "glue", subset_name: "mrpc", template_name: "equivalent"},
        glue_mrpc_equivalent_score_eval: {dataset_name: "glue", subset_name: "mrpc", template_name: "equivalent_score_eval"},
        glue_mrpc_generate_paraphrase: {dataset_name: "glue", subset_name: "mrpc", template_name: "generate_paraphrase"},
        glue_mrpc_generate_paraphrase_score_eval: {dataset_name: "glue", subset_name: "mrpc", template_name: "generate_paraphrase_score_eval"},
        glue_mrpc_generate_sentence: {dataset_name: "glue", subset_name: "mrpc", template_name: "generate_sentence"},
        glue_mrpc_generate_sentence_score_eval: {dataset_name: "glue", subset_name: "mrpc", template_name: "generate_sentence_score_eval"},
        glue_mrpc_paraphrase: {dataset_name: "glue", subset_name: "mrpc", template_name: "paraphrase"},
        glue_mrpc_paraphrase_score_eval: {dataset_name: "glue", subset_name: "mrpc", template_name: "paraphrase_score_eval"},
        glue_mrpc_replace: {dataset_name: "glue", subset_name: "mrpc", template_name: "replace"},
        glue_mrpc_replace_score_eval: {dataset_name: "glue", subset_name: "mrpc", template_name: "replace_score_eval"},
        glue_mrpc_same_thing: {dataset_name: "glue", subset_name: "mrpc", template_name: "same thing"},
        glue_mrpc_same_thing_score_eval: {dataset_name: "glue", subset_name: "mrpc", template_name: "same thing_score_eval"},
        glue_mrpc_want_to_know: {dataset_name: "glue", subset_name: "mrpc", template_name: "want to know"},
        glue_mrpc_want_to_know_score_eval: {dataset_name: "glue", subset_name: "mrpc", template_name: "want to know_score_eval"},
        glue_qqp_answer: {dataset_name: "glue", subset_name: "qqp", template_name: "answer"},
        glue_qqp_answer_score_eval: {dataset_name: "glue", subset_name: "qqp", template_name: "answer_score_eval"},
        glue_qqp_duplicate: {dataset_name: "glue", subset_name: "qqp", template_name: "duplicate"},
        glue_qqp_duplicate_or_not: {dataset_name: "glue", subset_name: "qqp", template_name: "duplicate or not"},
        glue_qqp_duplicate_or_not_score_eval: {dataset_name: "glue", subset_name: "qqp", template_name: "duplicate or not_score_eval"},
        glue_qqp_duplicate_score_eval: {dataset_name: "glue", subset_name: "qqp", template_name: "duplicate_score_eval"},
        glue_qqp_meaning: {dataset_name: "glue", subset_name: "qqp", template_name: "meaning"},
        glue_qqp_meaning_score_eval: {dataset_name: "glue", subset_name: "qqp", template_name: "meaning_score_eval"},
        glue_qqp_quora: {dataset_name: "glue", subset_name: "qqp", template_name: "quora"},
        glue_qqp_quora_score_eval: {dataset_name: "glue", subset_name: "qqp", template_name: "quora_score_eval"},
        glue_qqp_same_thing: {dataset_name: "glue", subset_name: "qqp", template_name: "same thing"},
        glue_qqp_same_thing_score_eval: {dataset_name: "glue", subset_name: "qqp", template_name: "same thing_score_eval"},
        hans_GPT_3_style: {dataset_name: "hans", subset_name: null, template_name: "GPT-3 style"},
        hans_GPT_3_style_score_eval: {dataset_name: "hans", subset_name: null, template_name: "GPT-3 style_score_eval"},
        hans_MNLI_crowdsource: {dataset_name: "hans", subset_name: null, template_name: "MNLI crowdsource"},
        hans_MNLI_crowdsource_score_eval: {dataset_name: "hans", subset_name: null, template_name: "MNLI crowdsource_score_eval"},
        hans_based_on_the_previous_passage: {dataset_name: "hans", subset_name: null, template_name: "based on the previous passage"},
        hans_based_on_the_previous_passage_score_eval: {dataset_name: "hans", subset_name: null, template_name: "based on the previous passage_score_eval"},
        hans_can_we_infer: {dataset_name: "hans", subset_name: null, template_name: "can we infer"},
        hans_can_we_infer_score_eval: {dataset_name: "hans", subset_name: null, template_name: "can we infer_score_eval"},
        hans_does_it_follow_that: {dataset_name: "hans", subset_name: null, template_name: "does it follow that"},
        hans_does_it_follow_that_score_eval: {dataset_name: "hans", subset_name: null, template_name: "does it follow that_score_eval"},
        hans_does_this_imply: {dataset_name: "hans", subset_name: null, template_name: "does this imply"},
        hans_does_this_imply_score_eval: {dataset_name: "hans", subset_name: null, template_name: "does this imply_score_eval"},
        hans_guaranteed_true: {dataset_name: "hans", subset_name: null, template_name: "guaranteed true"},
        hans_guaranteed_true_score_eval: {dataset_name: "hans", subset_name: null, template_name: "guaranteed true_score_eval"},
        hans_justified_in_saying: {dataset_name: "hans", subset_name: null, template_name: "justified in saying"},
        hans_justified_in_saying_score_eval: {dataset_name: "hans", subset_name: null, template_name: "justified in saying_score_eval"},
        hans_must_be_true: {dataset_name: "hans", subset_name: null, template_name: "must be true"},
        hans_must_be_true_score_eval: {dataset_name: "hans", subset_name: null, template_name: "must be true_score_eval"},
        hans_should_assume: {dataset_name: "hans", subset_name: null, template_name: "should assume"},
        hans_should_assume_score_eval: {dataset_name: "hans", subset_name: null, template_name: "should assume_score_eval"},
        hellaswag_Appropriate_continuation_Yes_or_No: {dataset_name: "hellaswag", subset_name: null, template_name: "Appropriate continuation - Yes or No"},
        hellaswag_Appropriate_continuation_Yes_or_No_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Appropriate continuation - Yes or No_score_eval"},
        hellaswag_Open_ended_completion: {dataset_name: "hellaswag", subset_name: null, template_name: "Open-ended completion"},
        hellaswag_Open_ended_completion_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Open-ended completion_score_eval"},
        hellaswag_Open_ended_start: {dataset_name: "hellaswag", subset_name: null, template_name: "Open-ended start"},
        hellaswag_Open_ended_start_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Open-ended start_score_eval"},
        hellaswag_Predict_ending_with_hint: {dataset_name: "hellaswag", subset_name: null, template_name: "Predict ending with hint"},
        hellaswag_Predict_ending_with_hint_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Predict ending with hint_score_eval"},
        hellaswag_Randomized_prompts_template: {dataset_name: "hellaswag", subset_name: null, template_name: "Randomized prompts template"},
        hellaswag_Randomized_prompts_template_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Randomized prompts template_score_eval"},
        hellaswag_Reversed_appropriate_continuation_Yes_or_No: {dataset_name: "hellaswag", subset_name: null, template_name: "Reversed appropriate continuation - Yes or No"},
        hellaswag_Reversed_appropriate_continuation_Yes_or_No_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Reversed appropriate continuation - Yes or No_score_eval"},
        hellaswag_Topic_of_the_context: {dataset_name: "hellaswag", subset_name: null, template_name: "Topic of the context"},
        hellaswag_Topic_of_the_context_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Topic of the context_score_eval"},
        hellaswag_Topic_without_the_ending_answer: {dataset_name: "hellaswag", subset_name: null, template_name: "Topic without the ending answer"},
        hellaswag_Topic_without_the_ending_answer_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "Topic without the ending answer_score_eval"},
        hellaswag_complete_first_then: {dataset_name: "hellaswag", subset_name: null, template_name: "complete_first_then"},
        hellaswag_complete_first_then_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "complete_first_then_score_eval"},
        hellaswag_how_ends: {dataset_name: "hellaswag", subset_name: null, template_name: "how_ends"},
        hellaswag_how_ends_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "how_ends_score_eval"},
        hellaswag_if_begins_how_continues: {dataset_name: "hellaswag", subset_name: null, template_name: "if_begins_how_continues"},
        hellaswag_if_begins_how_continues_score_eval: {dataset_name: "hellaswag", subset_name: null, template_name: "if_begins_how_continues_score_eval"},
        imdb_Movie_Expressed_Sentiment: {dataset_name: "imdb", subset_name: null, template_name: "Movie Expressed Sentiment"},
        imdb_Movie_Expressed_Sentiment_2: {dataset_name: "imdb", subset_name: null, template_name: "Movie Expressed Sentiment 2"},
        imdb_Movie_Expressed_Sentiment_2_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Movie Expressed Sentiment 2_score_eval"},
        imdb_Movie_Expressed_Sentiment_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Movie Expressed Sentiment_score_eval"},
        imdb_Negation_template_for_positive_and_negative: {dataset_name: "imdb", subset_name: null, template_name: "Negation template for positive and negative"},
        imdb_Negation_template_for_positive_and_negative_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Negation template for positive and negative_score_eval"},
        imdb_Reviewer_Enjoyment: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Enjoyment"},
        imdb_Reviewer_Enjoyment_Yes_No: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Enjoyment Yes No"},
        imdb_Reviewer_Enjoyment_Yes_No_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Enjoyment Yes No_score_eval"},
        imdb_Reviewer_Enjoyment_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Enjoyment_score_eval"},
        imdb_Reviewer_Expressed_Sentiment: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Expressed Sentiment"},
        imdb_Reviewer_Expressed_Sentiment_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Expressed Sentiment_score_eval"},
        imdb_Reviewer_Opinion_bad_good_choices: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Opinion bad good choices"},
        imdb_Reviewer_Opinion_bad_good_choices_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Opinion bad good choices_score_eval"},
        imdb_Reviewer_Sentiment_Feeling: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Sentiment Feeling"},
        imdb_Reviewer_Sentiment_Feeling_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Reviewer Sentiment Feeling_score_eval"},
        imdb_Sentiment_with_choices_: {dataset_name: "imdb", subset_name: null, template_name: "Sentiment with choices "},
        imdb_Sentiment_with_choices__score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Sentiment with choices _score_eval"},
        imdb_Text_Expressed_Sentiment: {dataset_name: "imdb", subset_name: null, template_name: "Text Expressed Sentiment"},
        imdb_Text_Expressed_Sentiment_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Text Expressed Sentiment_score_eval"},
        imdb_Writer_Expressed_Sentiment: {dataset_name: "imdb", subset_name: null, template_name: "Writer Expressed Sentiment"},
        imdb_Writer_Expressed_Sentiment_score_eval: {dataset_name: "imdb", subset_name: null, template_name: "Writer Expressed Sentiment_score_eval"},
        kilt_tasks_hotpotqa_combining_facts: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "combining_facts"},
        kilt_tasks_hotpotqa_combining_facts_score_eval: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "combining_facts_score_eval"},
        kilt_tasks_hotpotqa_complex_question: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "complex_question"},
        kilt_tasks_hotpotqa_complex_question_score_eval: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "complex_question_score_eval"},
        kilt_tasks_hotpotqa_final_exam: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "final_exam"},
        kilt_tasks_hotpotqa_final_exam_score_eval: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "final_exam_score_eval"},
        kilt_tasks_hotpotqa_formulate: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "formulate"},
        kilt_tasks_hotpotqa_formulate_score_eval: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "formulate_score_eval"},
        kilt_tasks_hotpotqa_straighforward_qa: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "straighforward_qa"},
        kilt_tasks_hotpotqa_straighforward_qa_score_eval: {dataset_name: "kilt_tasks", subset_name: "hotpotqa", template_name: "straighforward_qa_score_eval"},
        lambada_GPT_3_style: {dataset_name: "lambada", subset_name: null, template_name: "GPT-3 style"},
        lambada_GPT_3_style_score_eval: {dataset_name: "lambada", subset_name: null, template_name: "GPT-3 style_score_eval"},
        lambada_ellipses: {dataset_name: "lambada", subset_name: null, template_name: "ellipses"},
        lambada_ellipses_score_eval: {dataset_name: "lambada", subset_name: null, template_name: "ellipses_score_eval"},
        lambada_fill_in_the_____: {dataset_name: "lambada", subset_name: null, template_name: "fill in the ____"},
        lambada_fill_in_the______score_eval: {dataset_name: "lambada", subset_name: null, template_name: "fill in the _____score_eval"},
        lambada_please_next_word: {dataset_name: "lambada", subset_name: null, template_name: "please next word"},
        lambada_please_next_word_score_eval: {dataset_name: "lambada", subset_name: null, template_name: "please next word_score_eval"},
        lambada_what_comes_next: {dataset_name: "lambada", subset_name: null, template_name: "what comes next"},
        lambada_what_comes_next_score_eval: {dataset_name: "lambada", subset_name: null, template_name: "what comes next_score_eval"},
        mc_taco_asked_my_friend: {dataset_name: "mc_taco", subset_name: null, template_name: "asked_my_friend"},
        mc_taco_asked_my_friend_doubt: {dataset_name: "mc_taco", subset_name: null, template_name: "asked_my_friend_doubt"},
        mc_taco_asked_my_friend_doubt_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "asked_my_friend_doubt_score_eval"},
        mc_taco_asked_my_friend_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "asked_my_friend_score_eval"},
        mc_taco_believable: {dataset_name: "mc_taco", subset_name: null, template_name: "believable"},
        mc_taco_believable_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "believable_score_eval"},
        mc_taco_formal_description: {dataset_name: "mc_taco", subset_name: null, template_name: "formal_description"},
        mc_taco_formal_description_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "formal_description_score_eval"},
        mc_taco_generate_answer_from_question_and_context: {dataset_name: "mc_taco", subset_name: null, template_name: "generate_answer_from_question_and_context"},
        mc_taco_generate_answer_from_question_and_context_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "generate_answer_from_question_and_context_score_eval"},
        mc_taco_observe_check_plausible_yes_no: {dataset_name: "mc_taco", subset_name: null, template_name: "observe_check_plausible_yes_no"},
        mc_taco_observe_check_plausible_yes_no_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "observe_check_plausible_yes_no_score_eval"},
        mc_taco_plausible_negated: {dataset_name: "mc_taco", subset_name: null, template_name: "plausible_negated"},
        mc_taco_plausible_negated_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "plausible_negated_score_eval"},
        mc_taco_plausible_true_false: {dataset_name: "mc_taco", subset_name: null, template_name: "plausible_true_false"},
        mc_taco_plausible_true_false_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "plausible_true_false_score_eval"},
        mc_taco_temporal_categories_no_choices: {dataset_name: "mc_taco", subset_name: null, template_name: "temporal_categories_no_choices"},
        mc_taco_temporal_categories_no_choices_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "temporal_categories_no_choices_score_eval"},
        mc_taco_temporal_categories_with_choices: {dataset_name: "mc_taco", subset_name: null, template_name: "temporal_categories_with_choices"},
        mc_taco_temporal_categories_with_choices_score_eval: {dataset_name: "mc_taco", subset_name: null, template_name: "temporal_categories_with_choices_score_eval"},
        multi_news_distill: {dataset_name: "multi_news", subset_name: null, template_name: "distill"},
        multi_news_distill_score_eval: {dataset_name: "multi_news", subset_name: null, template_name: "distill_score_eval"},
        multi_news_expand_reverse_task_: {dataset_name: "multi_news", subset_name: null, template_name: "expand (reverse task)"},
        multi_news_expand_reverse_task__score_eval: {dataset_name: "multi_news", subset_name: null, template_name: "expand (reverse task)_score_eval"},
        multi_news_summarize: {dataset_name: "multi_news", subset_name: null, template_name: "summarize"},
        multi_news_summarize_score_eval: {dataset_name: "multi_news", subset_name: null, template_name: "summarize_score_eval"},
        multi_news_summary_scenario: {dataset_name: "multi_news", subset_name: null, template_name: "summary scenario"},
        multi_news_summary_scenario_score_eval: {dataset_name: "multi_news", subset_name: null, template_name: "summary scenario_score_eval"},
        multi_news_synthesize: {dataset_name: "multi_news", subset_name: null, template_name: "synthesize"},
        multi_news_synthesize_score_eval: {dataset_name: "multi_news", subset_name: null, template_name: "synthesize_score_eval"},
        multi_news_what_are_the_key_points: {dataset_name: "multi_news", subset_name: null, template_name: "what are the key points"},
        multi_news_what_are_the_key_points_score_eval: {dataset_name: "multi_news", subset_name: null, template_name: "what are the key points_score_eval"},
        nq_open_first_person_context: {dataset_name: "nq_open", subset_name: null, template_name: "first_person_context"},
        nq_open_first_person_context_score_eval: {dataset_name: "nq_open", subset_name: null, template_name: "first_person_context_score_eval"},
        nq_open_formal_description: {dataset_name: "nq_open", subset_name: null, template_name: "formal_description"},
        nq_open_formal_description_score_eval: {dataset_name: "nq_open", subset_name: null, template_name: "formal_description_score_eval"},
        nq_open_guess_question: {dataset_name: "nq_open", subset_name: null, template_name: "guess_question"},
        nq_open_guess_question_score_eval: {dataset_name: "nq_open", subset_name: null, template_name: "guess_question_score_eval"},
        nq_open_question_answer: {dataset_name: "nq_open", subset_name: null, template_name: "question_answer"},
        nq_open_question_answer_score_eval: {dataset_name: "nq_open", subset_name: null, template_name: "question_answer_score_eval"},
        nq_open_question_with_instruction: {dataset_name: "nq_open", subset_name: null, template_name: "question_with_instruction"},
        nq_open_question_with_instruction_score_eval: {dataset_name: "nq_open", subset_name: null, template_name: "question_with_instruction_score_eval"},
        nq_open_search_query: {dataset_name: "nq_open", subset_name: null, template_name: "search query"},
        nq_open_search_query_score_eval: {dataset_name: "nq_open", subset_name: null, template_name: "search query_score_eval"},
        openbookqa_main_choices: {dataset_name: "openbookqa", subset_name: "main", template_name: "choices"},
        openbookqa_main_choices_score_eval: {dataset_name: "openbookqa", subset_name: "main", template_name: "choices_score_eval"},
        openbookqa_main_choose_an_answer_with_options: {dataset_name: "openbookqa", subset_name: "main", template_name: "choose_an_answer_with_options"},
        openbookqa_main_choose_an_answer_with_options_score_eval: {dataset_name: "openbookqa", subset_name: "main", template_name: "choose_an_answer_with_options_score_eval"},
        openbookqa_main_only_options: {dataset_name: "openbookqa", subset_name: "main", template_name: "only_options"},
        openbookqa_main_only_options_score_eval: {dataset_name: "openbookqa", subset_name: "main", template_name: "only_options_score_eval"},
        openbookqa_main_pick_answer_with_options: {dataset_name: "openbookqa", subset_name: "main", template_name: "pick_answer_with_options"},
        openbookqa_main_pick_answer_with_options_score_eval: {dataset_name: "openbookqa", subset_name: "main", template_name: "pick_answer_with_options_score_eval"},
        openbookqa_main_pick_using_id: {dataset_name: "openbookqa", subset_name: "main", template_name: "pick_using_id"},
        openbookqa_main_pick_using_id_score_eval: {dataset_name: "openbookqa", subset_name: "main", template_name: "pick_using_id_score_eval"},
        openbookqa_main_which_correct: {dataset_name: "openbookqa", subset_name: "main", template_name: "which_correct"},
        openbookqa_main_which_correct_inverse: {dataset_name: "openbookqa", subset_name: "main", template_name: "which_correct_inverse"},
        openbookqa_main_which_correct_inverse_score_eval: {dataset_name: "openbookqa", subset_name: "main", template_name: "which_correct_inverse_score_eval"},
        openbookqa_main_which_correct_score_eval: {dataset_name: "openbookqa", subset_name: "main", template_name: "which_correct_score_eval"},
        paws_labeled_final_Concatenation: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Concatenation"},
        paws_labeled_final_Concatenation_no_label: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Concatenation-no-label"},
        paws_labeled_final_Concatenation_no_label_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Concatenation-no-label_score_eval"},
        paws_labeled_final_Concatenation_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Concatenation_score_eval"},
        paws_labeled_final_Meaning: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Meaning"},
        paws_labeled_final_Meaning_no_label: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Meaning-no-label"},
        paws_labeled_final_Meaning_no_label_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Meaning-no-label_score_eval"},
        paws_labeled_final_Meaning_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Meaning_score_eval"},
        paws_labeled_final_PAWS_ANLI_GPT3: {dataset_name: "paws", subset_name: "labeled_final", template_name: "PAWS-ANLI GPT3"},
        paws_labeled_final_PAWS_ANLI_GPT3_no_label: {dataset_name: "paws", subset_name: "labeled_final", template_name: "PAWS-ANLI GPT3-no-label"},
        paws_labeled_final_PAWS_ANLI_GPT3_no_label_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "PAWS-ANLI GPT3-no-label_score_eval"},
        paws_labeled_final_PAWS_ANLI_GPT3_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "PAWS-ANLI GPT3_score_eval"},
        paws_labeled_final_Rewrite: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Rewrite"},
        paws_labeled_final_Rewrite_no_label: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Rewrite-no-label"},
        paws_labeled_final_Rewrite_no_label_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Rewrite-no-label_score_eval"},
        paws_labeled_final_Rewrite_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "Rewrite_score_eval"},
        paws_labeled_final_context_question: {dataset_name: "paws", subset_name: "labeled_final", template_name: "context-question"},
        paws_labeled_final_context_question_no_label: {dataset_name: "paws", subset_name: "labeled_final", template_name: "context-question-no-label"},
        paws_labeled_final_context_question_no_label_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "context-question-no-label_score_eval"},
        paws_labeled_final_context_question_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "context-question_score_eval"},
        paws_labeled_final_paraphrase_task: {dataset_name: "paws", subset_name: "labeled_final", template_name: "paraphrase-task"},
        paws_labeled_final_paraphrase_task_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "paraphrase-task_score_eval"},
        paws_labeled_final_task_description_no_label: {dataset_name: "paws", subset_name: "labeled_final", template_name: "task_description-no-label"},
        paws_labeled_final_task_description_no_label_score_eval: {dataset_name: "paws", subset_name: "labeled_final", template_name: "task_description-no-label_score_eval"},
        piqa_Correct_the_solution: {dataset_name: "piqa", subset_name: null, template_name: "Correct the solution"},
        piqa_Correct_the_solution_if_false_from_sol_1: {dataset_name: "piqa", subset_name: null, template_name: "Correct the solution if false: from sol 1"},
        piqa_Correct_the_solution_if_false_from_sol_1_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "Correct the solution if false: from sol 1_score_eval"},
        piqa_Correct_the_solution_if_false_from_sol_2: {dataset_name: "piqa", subset_name: null, template_name: "Correct the solution if false: from sol 2"},
        piqa_Correct_the_solution_if_false_from_sol_2_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "Correct the solution if false: from sol 2_score_eval"},
        piqa_Correct_the_solution_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "Correct the solution_score_eval"},
        piqa_Does_this_solution_make_sense_sol1: {dataset_name: "piqa", subset_name: null, template_name: "Does this solution make sense? sol1"},
        piqa_Does_this_solution_make_sense_sol1_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "Does this solution make sense? sol1_score_eval"},
        piqa_Does_this_solution_make_sense_sol2: {dataset_name: "piqa", subset_name: null, template_name: "Does this solution make sense? sol2"},
        piqa_Does_this_solution_make_sense_sol2_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "Does this solution make sense? sol2_score_eval"},
        piqa_choose_the_most_appropriate_solution: {dataset_name: "piqa", subset_name: null, template_name: "choose the most appropriate solution"},
        piqa_choose_the_most_appropriate_solution_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "choose the most appropriate solution_score_eval"},
        piqa_finish_sentence_with_correct_choice: {dataset_name: "piqa", subset_name: null, template_name: "finish_sentence_with_correct_choice"},
        piqa_finish_sentence_with_correct_choice_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "finish_sentence_with_correct_choice_score_eval"},
        piqa_no_prompt_needed: {dataset_name: "piqa", subset_name: null, template_name: "no prompt needed"},
        piqa_no_prompt_needed_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "no prompt needed_score_eval"},
        piqa_pick_correct_choice_index: {dataset_name: "piqa", subset_name: null, template_name: "pick_correct_choice_index"},
        piqa_pick_correct_choice_index_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "pick_correct_choice_index_score_eval"},
        piqa_pick_correct_choice_with_choice_given_before_goal: {dataset_name: "piqa", subset_name: null, template_name: "pick_correct_choice_with_choice_given_before_goal"},
        piqa_pick_correct_choice_with_choice_given_before_goal_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "pick_correct_choice_with_choice_given_before_goal_score_eval"},
        piqa_what_is_the_correct_ending: {dataset_name: "piqa", subset_name: null, template_name: "what_is_the_correct_ending"},
        piqa_what_is_the_correct_ending_score_eval: {dataset_name: "piqa", subset_name: null, template_name: "what_is_the_correct_ending_score_eval"},
        qa_srl_answer_question: {dataset_name: "qa_srl", subset_name: null, template_name: "answer_question"},
        qa_srl_answer_question_score_eval: {dataset_name: "qa_srl", subset_name: null, template_name: "answer_question_score_eval"},
        qa_srl_deconstruct_sentence: {dataset_name: "qa_srl", subset_name: null, template_name: "deconstruct_sentence"},
        qa_srl_deconstruct_sentence_score_eval: {dataset_name: "qa_srl", subset_name: null, template_name: "deconstruct_sentence_score_eval"},
        qa_srl_generate_question: {dataset_name: "qa_srl", subset_name: null, template_name: "generate_question"},
        qa_srl_generate_question_score_eval: {dataset_name: "qa_srl", subset_name: null, template_name: "generate_question_score_eval"},
        qa_srl_identify_predicate: {dataset_name: "qa_srl", subset_name: null, template_name: "identify_predicate"},
        qa_srl_identify_predicate_score_eval: {dataset_name: "qa_srl", subset_name: null, template_name: "identify_predicate_score_eval"},
        qa_srl_linguistic_problem: {dataset_name: "qa_srl", subset_name: null, template_name: "linguistic_problem"},
        qa_srl_linguistic_problem_score_eval: {dataset_name: "qa_srl", subset_name: null, template_name: "linguistic_problem_score_eval"},
        qa_srl_parse_structure: {dataset_name: "qa_srl", subset_name: null, template_name: "parse_structure"},
        qa_srl_parse_structure_score_eval: {dataset_name: "qa_srl", subset_name: null, template_name: "parse_structure_score_eval"},
        qa_srl_sentence_question_concatenation: {dataset_name: "qa_srl", subset_name: null, template_name: "sentence_question_concatenation"},
        qa_srl_sentence_question_concatenation_score_eval: {dataset_name: "qa_srl", subset_name: null, template_name: "sentence_question_concatenation_score_eval"},
        qasc_is_correct_1: {dataset_name: "qasc", subset_name: null, template_name: "is_correct_1"},
        qasc_is_correct_1_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "is_correct_1_score_eval"},
        qasc_is_correct_2: {dataset_name: "qasc", subset_name: null, template_name: "is_correct_2"},
        qasc_is_correct_2_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "is_correct_2_score_eval"},
        qasc_qa_with_combined_facts_1: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_combined_facts_1"},
        qasc_qa_with_combined_facts_1_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_combined_facts_1_score_eval"},
        qasc_qa_with_separated_facts_1: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_1"},
        qasc_qa_with_separated_facts_1_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_1_score_eval"},
        qasc_qa_with_separated_facts_2: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_2"},
        qasc_qa_with_separated_facts_2_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_2_score_eval"},
        qasc_qa_with_separated_facts_3: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_3"},
        qasc_qa_with_separated_facts_3_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_3_score_eval"},
        qasc_qa_with_separated_facts_4: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_4"},
        qasc_qa_with_separated_facts_4_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_4_score_eval"},
        qasc_qa_with_separated_facts_5: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_5"},
        qasc_qa_with_separated_facts_5_score_eval: {dataset_name: "qasc", subset_name: null, template_name: "qa_with_separated_facts_5_score_eval"},
        quac_Answer_Converation_: {dataset_name: "quac", subset_name: null, template_name: "Answer Converation "},
        quac_Answer_Converation__score_eval: {dataset_name: "quac", subset_name: null, template_name: "Answer Converation _score_eval"},
        quac_Answer_Given_Full_Dialogue: {dataset_name: "quac", subset_name: null, template_name: "Answer Given Full Dialogue"},
        quac_Answer_Given_Full_Dialogue_score_eval: {dataset_name: "quac", subset_name: null, template_name: "Answer Given Full Dialogue_score_eval"},
        quac_Answer_Given_Only_First_Dialogue: {dataset_name: "quac", subset_name: null, template_name: "Answer Given Only First Dialogue"},
        quac_Answer_Given_Only_First_Dialogue_score_eval: {dataset_name: "quac", subset_name: null, template_name: "Answer Given Only First Dialogue_score_eval"},
        quac_Context_First_: {dataset_name: "quac", subset_name: null, template_name: "Context First "},
        quac_Context_First__score_eval: {dataset_name: "quac", subset_name: null, template_name: "Context First _score_eval"},
        quac_Student_Asking_Teacher_: {dataset_name: "quac", subset_name: null, template_name: "Student Asking Teacher "},
        quac_Student_Asking_Teacher__score_eval: {dataset_name: "quac", subset_name: null, template_name: "Student Asking Teacher _score_eval"},
        quac_Use_Dialogue_as_Hint: {dataset_name: "quac", subset_name: null, template_name: "Use Dialogue as Hint"},
        quac_Use_Dialogue_as_Hint_score_eval: {dataset_name: "quac", subset_name: null, template_name: "Use Dialogue as Hint_score_eval"},
        quail_context_description_question_answer_id: {dataset_name: "quail", subset_name: null, template_name: "context_description_question_answer_id"},
        quail_context_description_question_answer_id_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_description_question_answer_id_score_eval"},
        quail_context_description_question_answer_text: {dataset_name: "quail", subset_name: null, template_name: "context_description_question_answer_text"},
        quail_context_description_question_answer_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_description_question_answer_text_score_eval"},
        quail_context_description_question_text: {dataset_name: "quail", subset_name: null, template_name: "context_description_question_text"},
        quail_context_description_question_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_description_question_text_score_eval"},
        quail_context_question_answer_description_id: {dataset_name: "quail", subset_name: null, template_name: "context_question_answer_description_id"},
        quail_context_question_answer_description_id_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_question_answer_description_id_score_eval"},
        quail_context_question_answer_description_text: {dataset_name: "quail", subset_name: null, template_name: "context_question_answer_description_text"},
        quail_context_question_answer_description_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_question_answer_description_text_score_eval"},
        quail_context_question_description_answer_id: {dataset_name: "quail", subset_name: null, template_name: "context_question_description_answer_id"},
        quail_context_question_description_answer_id_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_question_description_answer_id_score_eval"},
        quail_context_question_description_answer_text: {dataset_name: "quail", subset_name: null, template_name: "context_question_description_answer_text"},
        quail_context_question_description_answer_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_question_description_answer_text_score_eval"},
        quail_context_question_description_text: {dataset_name: "quail", subset_name: null, template_name: "context_question_description_text"},
        quail_context_question_description_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "context_question_description_text_score_eval"},
        quail_description_context_question_answer_id: {dataset_name: "quail", subset_name: null, template_name: "description_context_question_answer_id"},
        quail_description_context_question_answer_id_score_eval: {dataset_name: "quail", subset_name: null, template_name: "description_context_question_answer_id_score_eval"},
        quail_description_context_question_answer_text: {dataset_name: "quail", subset_name: null, template_name: "description_context_question_answer_text"},
        quail_description_context_question_answer_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "description_context_question_answer_text_score_eval"},
        quail_description_context_question_text: {dataset_name: "quail", subset_name: null, template_name: "description_context_question_text"},
        quail_description_context_question_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "description_context_question_text_score_eval"},
        quail_no_prompt_id: {dataset_name: "quail", subset_name: null, template_name: "no_prompt_id"},
        quail_no_prompt_id_score_eval: {dataset_name: "quail", subset_name: null, template_name: "no_prompt_id_score_eval"},
        quail_no_prompt_text: {dataset_name: "quail", subset_name: null, template_name: "no_prompt_text"},
        quail_no_prompt_text_score_eval: {dataset_name: "quail", subset_name: null, template_name: "no_prompt_text_score_eval"},
        quarel_choose_between: {dataset_name: "quarel", subset_name: null, template_name: "choose_between"},
        quarel_choose_between_score_eval: {dataset_name: "quarel", subset_name: null, template_name: "choose_between_score_eval"},
        quarel_do_not_use: {dataset_name: "quarel", subset_name: null, template_name: "do_not_use"},
        quarel_do_not_use_score_eval: {dataset_name: "quarel", subset_name: null, template_name: "do_not_use_score_eval"},
        quarel_heres_a_story: {dataset_name: "quarel", subset_name: null, template_name: "heres_a_story"},
        quarel_heres_a_story_score_eval: {dataset_name: "quarel", subset_name: null, template_name: "heres_a_story_score_eval"},
        quarel_logic_test: {dataset_name: "quarel", subset_name: null, template_name: "logic_test"},
        quarel_logic_test_score_eval: {dataset_name: "quarel", subset_name: null, template_name: "logic_test_score_eval"},
        quarel_testing_students: {dataset_name: "quarel", subset_name: null, template_name: "testing_students"},
        quarel_testing_students_score_eval: {dataset_name: "quarel", subset_name: null, template_name: "testing_students_score_eval"},
        quartz_answer_question_based_on: {dataset_name: "quartz", subset_name: null, template_name: "answer_question_based_on"},
        quartz_answer_question_based_on_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "answer_question_based_on_score_eval"},
        quartz_answer_question_below: {dataset_name: "quartz", subset_name: null, template_name: "answer_question_below"},
        quartz_answer_question_below_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "answer_question_below_score_eval"},
        quartz_given_the_fact_answer_the_q: {dataset_name: "quartz", subset_name: null, template_name: "given_the_fact_answer_the_q"},
        quartz_given_the_fact_answer_the_q_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "given_the_fact_answer_the_q_score_eval"},
        quartz_having_read_above_passage: {dataset_name: "quartz", subset_name: null, template_name: "having_read_above_passage"},
        quartz_having_read_above_passage_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "having_read_above_passage_score_eval"},
        quartz_paragraph_question_plain_concat: {dataset_name: "quartz", subset_name: null, template_name: "paragraph_question_plain_concat"},
        quartz_paragraph_question_plain_concat_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "paragraph_question_plain_concat_score_eval"},
        quartz_read_passage_below_choose: {dataset_name: "quartz", subset_name: null, template_name: "read_passage_below_choose"},
        quartz_read_passage_below_choose_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "read_passage_below_choose_score_eval"},
        quartz_use_info_from_paragraph_question: {dataset_name: "quartz", subset_name: null, template_name: "use_info_from_paragraph_question"},
        quartz_use_info_from_paragraph_question_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "use_info_from_paragraph_question_score_eval"},
        quartz_use_info_from_question_paragraph: {dataset_name: "quartz", subset_name: null, template_name: "use_info_from_question_paragraph"},
        quartz_use_info_from_question_paragraph_score_eval: {dataset_name: "quartz", subset_name: null, template_name: "use_info_from_question_paragraph_score_eval"},
        quoref_Answer_Friend_Question: {dataset_name: "quoref", subset_name: null, template_name: "Answer Friend Question"},
        quoref_Answer_Friend_Question_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Answer Friend Question_score_eval"},
        quoref_Answer_Question_Given_Context: {dataset_name: "quoref", subset_name: null, template_name: "Answer Question Given Context"},
        quoref_Answer_Question_Given_Context_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Answer Question Given Context_score_eval"},
        quoref_Answer_Test: {dataset_name: "quoref", subset_name: null, template_name: "Answer Test"},
        quoref_Answer_Test_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Answer Test_score_eval"},
        quoref_Context_Contains_Answer: {dataset_name: "quoref", subset_name: null, template_name: "Context Contains Answer"},
        quoref_Context_Contains_Answer_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Context Contains Answer_score_eval"},
        quoref_Find_Answer: {dataset_name: "quoref", subset_name: null, template_name: "Find Answer"},
        quoref_Find_Answer_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Find Answer_score_eval"},
        quoref_Found_Context_Online: {dataset_name: "quoref", subset_name: null, template_name: "Found Context Online"},
        quoref_Found_Context_Online_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Found Context Online_score_eval"},
        quoref_Given_Context_Answer_Question: {dataset_name: "quoref", subset_name: null, template_name: "Given Context Answer Question"},
        quoref_Given_Context_Answer_Question_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Given Context Answer Question_score_eval"},
        quoref_Guess_Answer: {dataset_name: "quoref", subset_name: null, template_name: "Guess Answer"},
        quoref_Guess_Answer_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Guess Answer_score_eval"},
        quoref_Guess_Title_For_Context: {dataset_name: "quoref", subset_name: null, template_name: "Guess Title For Context"},
        quoref_Guess_Title_For_Context_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Guess Title For Context_score_eval"},
        quoref_Read_And_Extract_: {dataset_name: "quoref", subset_name: null, template_name: "Read And Extract "},
        quoref_Read_And_Extract__score_eval: {dataset_name: "quoref", subset_name: null, template_name: "Read And Extract _score_eval"},
        quoref_What_Is_The_Answer: {dataset_name: "quoref", subset_name: null, template_name: "What Is The Answer"},
        quoref_What_Is_The_Answer_score_eval: {dataset_name: "quoref", subset_name: null, template_name: "What Is The Answer_score_eval"},
        race_high_Is_this_the_right_answer: {dataset_name: "race", subset_name: "high", template_name: "Is this the right answer"},
        race_high_Is_this_the_right_answer_score_eval: {dataset_name: "race", subset_name: "high", template_name: "Is this the right answer_score_eval"},
        race_high_Read_the_article_and_answer_the_question_no_option_: {dataset_name: "race", subset_name: "high", template_name: "Read the article and answer the question (no option)"},
        race_high_Read_the_article_and_answer_the_question_no_option__score_eval: {dataset_name: "race", subset_name: "high", template_name: "Read the article and answer the question (no option)_score_eval"},
        race_high_Select_the_best_answer: {dataset_name: "race", subset_name: "high", template_name: "Select the best answer"},
        race_high_Select_the_best_answer_generate_span_: {dataset_name: "race", subset_name: "high", template_name: "Select the best answer (generate span)"},
        race_high_Select_the_best_answer_generate_span__score_eval: {dataset_name: "race", subset_name: "high", template_name: "Select the best answer (generate span)_score_eval"},
        race_high_Select_the_best_answer_no_instructions_: {dataset_name: "race", subset_name: "high", template_name: "Select the best answer (no instructions)"},
        race_high_Select_the_best_answer_no_instructions__score_eval: {dataset_name: "race", subset_name: "high", template_name: "Select the best answer (no instructions)_score_eval"},
        race_high_Select_the_best_answer_score_eval: {dataset_name: "race", subset_name: "high", template_name: "Select the best answer_score_eval"},
        race_high_Taking_a_test: {dataset_name: "race", subset_name: "high", template_name: "Taking a test"},
        race_high_Taking_a_test_score_eval: {dataset_name: "race", subset_name: "high", template_name: "Taking a test_score_eval"},
        race_high_Write_a_multi_choice_question_for_the_following_article: {dataset_name: "race", subset_name: "high", template_name: "Write a multi-choice question for the following article"},
        race_high_Write_a_multi_choice_question_for_the_following_article_score_eval: {dataset_name: "race", subset_name: "high", template_name: "Write a multi-choice question for the following article_score_eval"},
        race_high_Write_a_multi_choice_question_options_given_: {dataset_name: "race", subset_name: "high", template_name: "Write a multi-choice question (options given)"},
        race_high_Write_a_multi_choice_question_options_given__score_eval: {dataset_name: "race", subset_name: "high", template_name: "Write a multi-choice question (options given)_score_eval"},
        race_middle_Is_this_the_right_answer: {dataset_name: "race", subset_name: "middle", template_name: "Is this the right answer"},
        race_middle_Is_this_the_right_answer_score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Is this the right answer_score_eval"},
        race_middle_Read_the_article_and_answer_the_question_no_option_: {dataset_name: "race", subset_name: "middle", template_name: "Read the article and answer the question (no option)"},
        race_middle_Read_the_article_and_answer_the_question_no_option__score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Read the article and answer the question (no option)_score_eval"},
        race_middle_Select_the_best_answer: {dataset_name: "race", subset_name: "middle", template_name: "Select the best answer"},
        race_middle_Select_the_best_answer_generate_span_: {dataset_name: "race", subset_name: "middle", template_name: "Select the best answer (generate span)"},
        race_middle_Select_the_best_answer_generate_span__score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Select the best answer (generate span)_score_eval"},
        race_middle_Select_the_best_answer_no_instructions_: {dataset_name: "race", subset_name: "middle", template_name: "Select the best answer (no instructions)"},
        race_middle_Select_the_best_answer_no_instructions__score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Select the best answer (no instructions)_score_eval"},
        race_middle_Select_the_best_answer_score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Select the best answer_score_eval"},
        race_middle_Taking_a_test: {dataset_name: "race", subset_name: "middle", template_name: "Taking a test"},
        race_middle_Taking_a_test_score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Taking a test_score_eval"},
        race_middle_Write_a_multi_choice_question_for_the_following_article: {dataset_name: "race", subset_name: "middle", template_name: "Write a multi-choice question for the following article"},
        race_middle_Write_a_multi_choice_question_for_the_following_article_score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Write a multi-choice question for the following article_score_eval"},
        race_middle_Write_a_multi_choice_question_options_given_: {dataset_name: "race", subset_name: "middle", template_name: "Write a multi-choice question (options given)"},
        race_middle_Write_a_multi_choice_question_options_given__score_eval: {dataset_name: "race", subset_name: "middle", template_name: "Write a multi-choice question (options given)_score_eval"},
        ropes_background_new_situation_answer: {dataset_name: "ropes", subset_name: null, template_name: "background_new_situation_answer"},
        ropes_background_new_situation_answer_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "background_new_situation_answer_score_eval"},
        ropes_background_situation_middle: {dataset_name: "ropes", subset_name: null, template_name: "background_situation_middle"},
        ropes_background_situation_middle_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "background_situation_middle_score_eval"},
        ropes_given_background_situation: {dataset_name: "ropes", subset_name: null, template_name: "given_background_situation"},
        ropes_given_background_situation_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "given_background_situation_score_eval"},
        ropes_new_situation_background_answer: {dataset_name: "ropes", subset_name: null, template_name: "new_situation_background_answer"},
        ropes_new_situation_background_answer_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "new_situation_background_answer_score_eval"},
        ropes_plain_background_situation: {dataset_name: "ropes", subset_name: null, template_name: "plain_background_situation"},
        ropes_plain_background_situation_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "plain_background_situation_score_eval"},
        ropes_plain_bottom_hint: {dataset_name: "ropes", subset_name: null, template_name: "plain_bottom_hint"},
        ropes_plain_bottom_hint_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "plain_bottom_hint_score_eval"},
        ropes_plain_no_background: {dataset_name: "ropes", subset_name: null, template_name: "plain_no_background"},
        ropes_plain_no_background_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "plain_no_background_score_eval"},
        ropes_prompt_beginning: {dataset_name: "ropes", subset_name: null, template_name: "prompt_beginning"},
        ropes_prompt_beginning_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "prompt_beginning_score_eval"},
        ropes_prompt_bottom_hint_beginning: {dataset_name: "ropes", subset_name: null, template_name: "prompt_bottom_hint_beginning"},
        ropes_prompt_bottom_hint_beginning_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "prompt_bottom_hint_beginning_score_eval"},
        ropes_prompt_bottom_no_hint: {dataset_name: "ropes", subset_name: null, template_name: "prompt_bottom_no_hint"},
        ropes_prompt_bottom_no_hint_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "prompt_bottom_no_hint_score_eval"},
        ropes_prompt_mix: {dataset_name: "ropes", subset_name: null, template_name: "prompt_mix"},
        ropes_prompt_mix_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "prompt_mix_score_eval"},
        ropes_read_background_situation: {dataset_name: "ropes", subset_name: null, template_name: "read_background_situation"},
        ropes_read_background_situation_score_eval: {dataset_name: "ropes", subset_name: null, template_name: "read_background_situation_score_eval"},
        rotten_tomatoes_Movie_Expressed_Sentiment: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Movie Expressed Sentiment"},
        rotten_tomatoes_Movie_Expressed_Sentiment_2: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Movie Expressed Sentiment 2"},
        rotten_tomatoes_Movie_Expressed_Sentiment_2_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Movie Expressed Sentiment 2_score_eval"},
        rotten_tomatoes_Movie_Expressed_Sentiment_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Movie Expressed Sentiment_score_eval"},
        rotten_tomatoes_Reviewer_Enjoyment: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Enjoyment"},
        rotten_tomatoes_Reviewer_Enjoyment_Yes_No: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Enjoyment Yes No"},
        rotten_tomatoes_Reviewer_Enjoyment_Yes_No_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Enjoyment Yes No_score_eval"},
        rotten_tomatoes_Reviewer_Enjoyment_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Enjoyment_score_eval"},
        rotten_tomatoes_Reviewer_Expressed_Sentiment: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Expressed Sentiment"},
        rotten_tomatoes_Reviewer_Expressed_Sentiment_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Expressed Sentiment_score_eval"},
        rotten_tomatoes_Reviewer_Opinion_bad_good_choices: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Opinion bad good choices"},
        rotten_tomatoes_Reviewer_Opinion_bad_good_choices_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Opinion bad good choices_score_eval"},
        rotten_tomatoes_Reviewer_Sentiment_Feeling: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Sentiment Feeling"},
        rotten_tomatoes_Reviewer_Sentiment_Feeling_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Reviewer Sentiment Feeling_score_eval"},
        rotten_tomatoes_Sentiment_with_choices_: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Sentiment with choices "},
        rotten_tomatoes_Sentiment_with_choices__score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Sentiment with choices _score_eval"},
        rotten_tomatoes_Text_Expressed_Sentiment: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Text Expressed Sentiment"},
        rotten_tomatoes_Text_Expressed_Sentiment_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Text Expressed Sentiment_score_eval"},
        rotten_tomatoes_Writer_Expressed_Sentiment: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Writer Expressed Sentiment"},
        rotten_tomatoes_Writer_Expressed_Sentiment_score_eval: {dataset_name: "rotten_tomatoes", subset_name: null, template_name: "Writer Expressed Sentiment_score_eval"},
        samsum_Generate_a_summary_for_this_dialogue: {dataset_name: "samsum", subset_name: null, template_name: "Generate a summary for this dialogue"},
        samsum_Generate_a_summary_for_this_dialogue_score_eval: {dataset_name: "samsum", subset_name: null, template_name: "Generate a summary for this dialogue_score_eval"},
        samsum_Given_the_above_dialogue_write_a_summary: {dataset_name: "samsum", subset_name: null, template_name: "Given the above dialogue write a summary"},
        samsum_Given_the_above_dialogue_write_a_summary_score_eval: {dataset_name: "samsum", subset_name: null, template_name: "Given the above dialogue write a summary_score_eval"},
        samsum_Sum_up_the_following_dialogue: {dataset_name: "samsum", subset_name: null, template_name: "Sum up the following dialogue"},
        samsum_Sum_up_the_following_dialogue_score_eval: {dataset_name: "samsum", subset_name: null, template_name: "Sum up the following dialogue_score_eval"},
        samsum_Summarize_: {dataset_name: "samsum", subset_name: null, template_name: "Summarize:"},
        samsum_Summarize__score_eval: {dataset_name: "samsum", subset_name: null, template_name: "Summarize:_score_eval"},
        samsum_Summarize_this_dialogue_: {dataset_name: "samsum", subset_name: null, template_name: "Summarize this dialogue:"},
        samsum_Summarize_this_dialogue__score_eval: {dataset_name: "samsum", subset_name: null, template_name: "Summarize this dialogue:_score_eval"},
        samsum_To_sum_up_this_dialog: {dataset_name: "samsum", subset_name: null, template_name: "To sum up this dialog"},
        samsum_To_sum_up_this_dialog_score_eval: {dataset_name: "samsum", subset_name: null, template_name: "To sum up this dialog_score_eval"},
        samsum_Write_a_dialogue_that_match_this_summary: {dataset_name: "samsum", subset_name: null, template_name: "Write a dialogue that match this summary"},
        samsum_Write_a_dialogue_that_match_this_summary_score_eval: {dataset_name: "samsum", subset_name: null, template_name: "Write a dialogue that match this summary_score_eval"},
        sciq_Direct_Question: {dataset_name: "sciq", subset_name: null, template_name: "Direct Question"},
        sciq_Direct_Question_Closed_Book_: {dataset_name: "sciq", subset_name: null, template_name: "Direct Question (Closed Book)"},
        sciq_Direct_Question_Closed_Book__score_eval: {dataset_name: "sciq", subset_name: null, template_name: "Direct Question (Closed Book)_score_eval"},
        sciq_Direct_Question_score_eval: {dataset_name: "sciq", subset_name: null, template_name: "Direct Question_score_eval"},
        sciq_Multiple_Choice: {dataset_name: "sciq", subset_name: null, template_name: "Multiple Choice"},
        sciq_Multiple_Choice_Closed_Book_: {dataset_name: "sciq", subset_name: null, template_name: "Multiple Choice (Closed Book)"},
        sciq_Multiple_Choice_Closed_Book__score_eval: {dataset_name: "sciq", subset_name: null, template_name: "Multiple Choice (Closed Book)_score_eval"},
        sciq_Multiple_Choice_Question_First: {dataset_name: "sciq", subset_name: null, template_name: "Multiple Choice Question First"},
        sciq_Multiple_Choice_Question_First_score_eval: {dataset_name: "sciq", subset_name: null, template_name: "Multiple Choice Question First_score_eval"},
        sciq_Multiple_Choice_score_eval: {dataset_name: "sciq", subset_name: null, template_name: "Multiple Choice_score_eval"},
        social_i_qa_Check_if_a_random_answer_is_valid_or_not: {dataset_name: "social_i_qa", subset_name: null, template_name: "Check if a random answer is valid or not"},
        social_i_qa_Check_if_a_random_answer_is_valid_or_not_score_eval: {dataset_name: "social_i_qa", subset_name: null, template_name: "Check if a random answer is valid or not_score_eval"},
        social_i_qa_Generate_answer: {dataset_name: "social_i_qa", subset_name: null, template_name: "Generate answer"},
        social_i_qa_Generate_answer_score_eval: {dataset_name: "social_i_qa", subset_name: null, template_name: "Generate answer_score_eval"},
        social_i_qa_Generate_the_question_from_the_answer: {dataset_name: "social_i_qa", subset_name: null, template_name: "Generate the question from the answer"},
        social_i_qa_Generate_the_question_from_the_answer_score_eval: {dataset_name: "social_i_qa", subset_name: null, template_name: "Generate the question from the answer_score_eval"},
        social_i_qa_I_was_wondering: {dataset_name: "social_i_qa", subset_name: null, template_name: "I was wondering"},
        social_i_qa_I_was_wondering_score_eval: {dataset_name: "social_i_qa", subset_name: null, template_name: "I was wondering_score_eval"},
        social_i_qa_Show_choices_and_generate_answer: {dataset_name: "social_i_qa", subset_name: null, template_name: "Show choices and generate answer"},
        social_i_qa_Show_choices_and_generate_answer_score_eval: {dataset_name: "social_i_qa", subset_name: null, template_name: "Show choices and generate answer_score_eval"},
        social_i_qa_Show_choices_and_generate_index: {dataset_name: "social_i_qa", subset_name: null, template_name: "Show choices and generate index"},
        social_i_qa_Show_choices_and_generate_index_score_eval: {dataset_name: "social_i_qa", subset_name: null, template_name: "Show choices and generate index_score_eval"},
        squad_v2_Jeopardy_with_Context: {dataset_name: "squad_v2", subset_name: null, template_name: "Jeopardy with Context"},
        squad_v2_Jeopardy_with_Context_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Jeopardy with Context_score_eval"},
        squad_v2_Jeopardy_without_Context: {dataset_name: "squad_v2", subset_name: null, template_name: "Jeopardy without Context"},
        squad_v2_Jeopardy_without_Context_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Jeopardy without Context_score_eval"},
        squad_v2_Questions_with_Context: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context"},
        squad_v2_Questions_with_Context_Without_Prompt_Keywords: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context - Without Prompt Keywords"},
        squad_v2_Questions_with_Context_Without_Prompt_Keywords_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context - Without Prompt Keywords_score_eval"},
        squad_v2_Questions_with_Context_Without_Prompt_Keywords_unanswerable: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context - Without Prompt Keywords +unanswerable"},
        squad_v2_Questions_with_Context_Without_Prompt_Keywords_unanswerable_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context - Without Prompt Keywords +unanswerable_score_eval"},
        squad_v2_Questions_with_Context_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context_score_eval"},
        squad_v2_Questions_with_Context_unanswerable: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context +unanswerable"},
        squad_v2_Questions_with_Context_unanswerable_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Questions with Context +unanswerable_score_eval"},
        squad_v2_Topic_Prediction_Context: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Context"},
        squad_v2_Topic_Prediction_Context_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Context_score_eval"},
        squad_v2_Topic_Prediction_Context_with_randomized_prompt_options: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Context with randomized prompt options"},
        squad_v2_Topic_Prediction_Context_with_randomized_prompt_options_placed_in_the_end: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Context with randomized prompt options placed in the end"},
        squad_v2_Topic_Prediction_Context_with_randomized_prompt_options_placed_in_the_end_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Context with randomized prompt options placed in the end_score_eval"},
        squad_v2_Topic_Prediction_Context_with_randomized_prompt_options_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Context with randomized prompt options_score_eval"},
        squad_v2_Topic_Prediction_Question_and_Answer_Pair: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Question and Answer Pair"},
        squad_v2_Topic_Prediction_Question_and_Answer_Pair_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Topic Prediction - Question and Answer Pair_score_eval"},
        squad_v2_Trivia: {dataset_name: "squad_v2", subset_name: null, template_name: "Trivia"},
        squad_v2_Trivia_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Trivia_score_eval"},
        squad_v2_Unanwerable_question: {dataset_name: "squad_v2", subset_name: null, template_name: "Unanwerable question"},
        squad_v2_Unanwerable_question_score_eval: {dataset_name: "squad_v2", subset_name: null, template_name: "Unanwerable question_score_eval"},
        story_cloze_2016_Answer_Given_options: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Answer Given options"},
        story_cloze_2016_Answer_Given_options_score_eval: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Answer Given options_score_eval"},
        story_cloze_2016_Choose_Story_Ending: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Choose Story Ending"},
        story_cloze_2016_Choose_Story_Ending_score_eval: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Choose Story Ending_score_eval"},
        story_cloze_2016_Generate_Ending: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Generate Ending"},
        story_cloze_2016_Generate_Ending_score_eval: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Generate Ending_score_eval"},
        story_cloze_2016_Movie_What_Happens_Next: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Movie What Happens Next"},
        story_cloze_2016_Movie_What_Happens_Next_score_eval: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Movie What Happens Next_score_eval"},
        story_cloze_2016_Novel_Correct_Ending: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Novel Correct Ending"},
        story_cloze_2016_Novel_Correct_Ending_score_eval: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Novel Correct Ending_score_eval"},
        story_cloze_2016_Story_Continuation_and_Options: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Story Continuation and Options"},
        story_cloze_2016_Story_Continuation_and_Options_score_eval: {dataset_name: "story_cloze", subset_name: "2016", template_name: "Story Continuation and Options_score_eval"},
        super_glue_axb_GPT_3_style: {dataset_name: "super_glue", subset_name: "axb", template_name: "GPT-3 style"},
        super_glue_axb_GPT_3_style_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "GPT-3 style_score_eval"},
        super_glue_axb_MNLI_crowdsource: {dataset_name: "super_glue", subset_name: "axb", template_name: "MNLI crowdsource"},
        super_glue_axb_MNLI_crowdsource_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "MNLI crowdsource_score_eval"},
        super_glue_axb_based_on_the_previous_passage: {dataset_name: "super_glue", subset_name: "axb", template_name: "based on the previous passage"},
        super_glue_axb_based_on_the_previous_passage_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "based on the previous passage_score_eval"},
        super_glue_axb_can_we_infer: {dataset_name: "super_glue", subset_name: "axb", template_name: "can we infer"},
        super_glue_axb_can_we_infer_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "can we infer_score_eval"},
        super_glue_axb_does_it_follow_that: {dataset_name: "super_glue", subset_name: "axb", template_name: "does it follow that"},
        super_glue_axb_does_it_follow_that_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "does it follow that_score_eval"},
        super_glue_axb_does_this_imply: {dataset_name: "super_glue", subset_name: "axb", template_name: "does this imply"},
        super_glue_axb_does_this_imply_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "does this imply_score_eval"},
        super_glue_axb_guaranteed_true: {dataset_name: "super_glue", subset_name: "axb", template_name: "guaranteed true"},
        super_glue_axb_guaranteed_true_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "guaranteed true_score_eval"},
        super_glue_axb_justified_in_saying: {dataset_name: "super_glue", subset_name: "axb", template_name: "justified in saying"},
        super_glue_axb_justified_in_saying_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "justified in saying_score_eval"},
        super_glue_axb_must_be_true: {dataset_name: "super_glue", subset_name: "axb", template_name: "must be true"},
        super_glue_axb_must_be_true_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "must be true_score_eval"},
        super_glue_axb_should_assume: {dataset_name: "super_glue", subset_name: "axb", template_name: "should assume"},
        super_glue_axb_should_assume_score_eval: {dataset_name: "super_glue", subset_name: "axb", template_name: "should assume_score_eval"},
        super_glue_axg_GPT_3_style: {dataset_name: "super_glue", subset_name: "axg", template_name: "GPT-3 style"},
        super_glue_axg_GPT_3_style_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "GPT-3 style_score_eval"},
        super_glue_axg_MNLI_crowdsource: {dataset_name: "super_glue", subset_name: "axg", template_name: "MNLI crowdsource"},
        super_glue_axg_MNLI_crowdsource_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "MNLI crowdsource_score_eval"},
        super_glue_axg_based_on_the_previous_passage: {dataset_name: "super_glue", subset_name: "axg", template_name: "based on the previous passage"},
        super_glue_axg_based_on_the_previous_passage_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "based on the previous passage_score_eval"},
        super_glue_axg_can_we_infer: {dataset_name: "super_glue", subset_name: "axg", template_name: "can we infer"},
        super_glue_axg_can_we_infer_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "can we infer_score_eval"},
        super_glue_axg_does_it_follow_that: {dataset_name: "super_glue", subset_name: "axg", template_name: "does it follow that"},
        super_glue_axg_does_it_follow_that_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "does it follow that_score_eval"},
        super_glue_axg_does_this_imply: {dataset_name: "super_glue", subset_name: "axg", template_name: "does this imply"},
        super_glue_axg_does_this_imply_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "does this imply_score_eval"},
        super_glue_axg_guaranteed_true: {dataset_name: "super_glue", subset_name: "axg", template_name: "guaranteed true"},
        super_glue_axg_guaranteed_true_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "guaranteed true_score_eval"},
        super_glue_axg_justified_in_saying: {dataset_name: "super_glue", subset_name: "axg", template_name: "justified in saying"},
        super_glue_axg_justified_in_saying_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "justified in saying_score_eval"},
        super_glue_axg_must_be_true: {dataset_name: "super_glue", subset_name: "axg", template_name: "must be true"},
        super_glue_axg_must_be_true_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "must be true_score_eval"},
        super_glue_axg_should_assume: {dataset_name: "super_glue", subset_name: "axg", template_name: "should assume"},
        super_glue_axg_should_assume_score_eval: {dataset_name: "super_glue", subset_name: "axg", template_name: "should assume_score_eval"},
        super_glue_boolq_GPT_3_Style: {dataset_name: "super_glue", subset_name: "boolq", template_name: "GPT-3 Style"},
        super_glue_boolq_GPT_3_Style_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "GPT-3 Style_score_eval"},
        super_glue_boolq_I_wonder_: {dataset_name: "super_glue", subset_name: "boolq", template_name: "I wonder"},
        super_glue_boolq_I_wonder__score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "I wonder_score_eval"},
        super_glue_boolq_after_reading: {dataset_name: "super_glue", subset_name: "boolq", template_name: "after_reading"},
        super_glue_boolq_after_reading_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "after_reading_score_eval"},
        super_glue_boolq_based_on_the_following_passage: {dataset_name: "super_glue", subset_name: "boolq", template_name: "based on the following passage"},
        super_glue_boolq_based_on_the_following_passage_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "based on the following passage_score_eval"},
        super_glue_boolq_based_on_the_previous_passage: {dataset_name: "super_glue", subset_name: "boolq", template_name: "based on the previous passage"},
        super_glue_boolq_based_on_the_previous_passage_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "based on the previous passage_score_eval"},
        super_glue_boolq_could_you_tell_me_: {dataset_name: "super_glue", subset_name: "boolq", template_name: "could you tell me"},
        super_glue_boolq_could_you_tell_me__score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "could you tell me_score_eval"},
        super_glue_boolq_exam: {dataset_name: "super_glue", subset_name: "boolq", template_name: "exam"},
        super_glue_boolq_exam_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "exam_score_eval"},
        super_glue_boolq_exercise: {dataset_name: "super_glue", subset_name: "boolq", template_name: "exercise"},
        super_glue_boolq_exercise_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "exercise_score_eval"},
        super_glue_boolq_valid_binary: {dataset_name: "super_glue", subset_name: "boolq", template_name: "valid_binary"},
        super_glue_boolq_valid_binary_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "valid_binary_score_eval"},
        super_glue_boolq_yes_no_question: {dataset_name: "super_glue", subset_name: "boolq", template_name: "yes_no_question"},
        super_glue_boolq_yes_no_question_score_eval: {dataset_name: "super_glue", subset_name: "boolq", template_name: "yes_no_question_score_eval"},
        super_glue_cb_GPT_3_style: {dataset_name: "super_glue", subset_name: "cb", template_name: "GPT-3 style"},
        super_glue_cb_GPT_3_style_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "GPT-3 style_score_eval"},
        super_glue_cb_MNLI_crowdsource: {dataset_name: "super_glue", subset_name: "cb", template_name: "MNLI crowdsource"},
        super_glue_cb_MNLI_crowdsource_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "MNLI crowdsource_score_eval"},
        super_glue_cb_always_sometimes_never: {dataset_name: "super_glue", subset_name: "cb", template_name: "always/sometimes/never"},
        super_glue_cb_always_sometimes_never_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "always/sometimes/never_score_eval"},
        super_glue_cb_based_on_the_previous_passage: {dataset_name: "super_glue", subset_name: "cb", template_name: "based on the previous passage"},
        super_glue_cb_based_on_the_previous_passage_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "based on the previous passage_score_eval"},
        super_glue_cb_can_we_infer: {dataset_name: "super_glue", subset_name: "cb", template_name: "can we infer"},
        super_glue_cb_can_we_infer_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "can we infer_score_eval"},
        super_glue_cb_claim_true_false_inconclusive: {dataset_name: "super_glue", subset_name: "cb", template_name: "claim true/false/inconclusive"},
        super_glue_cb_claim_true_false_inconclusive_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "claim true/false/inconclusive_score_eval"},
        super_glue_cb_consider_always_sometimes_never: {dataset_name: "super_glue", subset_name: "cb", template_name: "consider always/sometimes/never"},
        super_glue_cb_consider_always_sometimes_never_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "consider always/sometimes/never_score_eval"},
        super_glue_cb_does_it_follow_that: {dataset_name: "super_glue", subset_name: "cb", template_name: "does it follow that"},
        super_glue_cb_does_it_follow_that_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "does it follow that_score_eval"},
        super_glue_cb_does_this_imply: {dataset_name: "super_glue", subset_name: "cb", template_name: "does this imply"},
        super_glue_cb_does_this_imply_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "does this imply_score_eval"},
        super_glue_cb_guaranteed_possible_impossible: {dataset_name: "super_glue", subset_name: "cb", template_name: "guaranteed/possible/impossible"},
        super_glue_cb_guaranteed_possible_impossible_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "guaranteed/possible/impossible_score_eval"},
        super_glue_cb_guaranteed_true: {dataset_name: "super_glue", subset_name: "cb", template_name: "guaranteed true"},
        super_glue_cb_guaranteed_true_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "guaranteed true_score_eval"},
        super_glue_cb_justified_in_saying: {dataset_name: "super_glue", subset_name: "cb", template_name: "justified in saying"},
        super_glue_cb_justified_in_saying_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "justified in saying_score_eval"},
        super_glue_cb_must_be_true: {dataset_name: "super_glue", subset_name: "cb", template_name: "must be true"},
        super_glue_cb_must_be_true_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "must be true_score_eval"},
        super_glue_cb_should_assume: {dataset_name: "super_glue", subset_name: "cb", template_name: "should assume"},
        super_glue_cb_should_assume_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "should assume_score_eval"},
        super_glue_cb_take_the_following_as_truth: {dataset_name: "super_glue", subset_name: "cb", template_name: "take the following as truth"},
        super_glue_cb_take_the_following_as_truth_score_eval: {dataset_name: "super_glue", subset_name: "cb", template_name: "take the following as truth_score_eval"},
        super_glue_copa_C1_or_C2_premise_so_because_: {dataset_name: "super_glue", subset_name: "copa", template_name: "C1 or C2? premise, so/because"},
        super_glue_copa_C1_or_C2_premise_so_because__score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "C1 or C2? premise, so/because_score_eval"},
        super_glue_copa__As_a_result_C1_or_C2_: {dataset_name: "super_glue", subset_name: "copa", template_name: "As a result, C1 or C2?"},
        super_glue_copa__As_a_result_C1_or_C2__score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "As a result, C1 or C2?_score_eval"},
        super_glue_copa__What_could_happen_next_C1_or_C2_: {dataset_name: "super_glue", subset_name: "copa", template_name: "What could happen next, C1 or C2?"},
        super_glue_copa__What_could_happen_next_C1_or_C2__score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "What could happen next, C1 or C2?_score_eval"},
        super_glue_copa__which_may_be_caused_by: {dataset_name: "super_glue", subset_name: "copa", template_name: "which may be caused by"},
        super_glue_copa__which_may_be_caused_by_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "which may be caused by_score_eval"},
        super_glue_copa__why_C1_or_C2: {dataset_name: "super_glue", subset_name: "copa", template_name: "why? C1 or C2"},
        super_glue_copa__why_C1_or_C2_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "why? C1 or C2_score_eval"},
        super_glue_copa_best_option: {dataset_name: "super_glue", subset_name: "copa", template_name: "best_option"},
        super_glue_copa_best_option_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "best_option_score_eval"},
        super_glue_copa_cause_effect: {dataset_name: "super_glue", subset_name: "copa", template_name: "cause_effect"},
        super_glue_copa_cause_effect_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "cause_effect_score_eval"},
        super_glue_copa_choose: {dataset_name: "super_glue", subset_name: "copa", template_name: "choose"},
        super_glue_copa_choose_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "choose_score_eval"},
        super_glue_copa_exercise: {dataset_name: "super_glue", subset_name: "copa", template_name: "exercise"},
        super_glue_copa_exercise_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "exercise_score_eval"},
        super_glue_copa_i_am_hesitating: {dataset_name: "super_glue", subset_name: "copa", template_name: "i_am_hesitating"},
        super_glue_copa_i_am_hesitating_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "i_am_hesitating_score_eval"},
        super_glue_copa_more_likely: {dataset_name: "super_glue", subset_name: "copa", template_name: "more likely"},
        super_glue_copa_more_likely_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "more likely_score_eval"},
        super_glue_copa_plausible_alternatives: {dataset_name: "super_glue", subset_name: "copa", template_name: "plausible_alternatives"},
        super_glue_copa_plausible_alternatives_score_eval: {dataset_name: "super_glue", subset_name: "copa", template_name: "plausible_alternatives_score_eval"},
        super_glue_multirc_I_was_going_to_say_: {dataset_name: "super_glue", subset_name: "multirc", template_name: "I was going to say"},
        super_glue_multirc_I_was_going_to_say__score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "I was going to say_score_eval"},
        super_glue_multirc_Would_it_be_good_to_answer_: {dataset_name: "super_glue", subset_name: "multirc", template_name: "Would it be good to answer"},
        super_glue_multirc_Would_it_be_good_to_answer__score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "Would it be good to answer_score_eval"},
        super_glue_multirc_confirm: {dataset_name: "super_glue", subset_name: "multirc", template_name: "confirm"},
        super_glue_multirc_confirm_score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "confirm_score_eval"},
        super_glue_multirc_correct: {dataset_name: "super_glue", subset_name: "multirc", template_name: "correct"},
        super_glue_multirc_correct_score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "correct_score_eval"},
        super_glue_multirc_decide_valid: {dataset_name: "super_glue", subset_name: "multirc", template_name: "decide_valid"},
        super_glue_multirc_decide_valid_score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "decide_valid_score_eval"},
        super_glue_multirc_found_this_answer: {dataset_name: "super_glue", subset_name: "multirc", template_name: "found_this_answer"},
        super_glue_multirc_found_this_answer_score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "found_this_answer_score_eval"},
        super_glue_multirc_grading: {dataset_name: "super_glue", subset_name: "multirc", template_name: "grading"},
        super_glue_multirc_grading_score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "grading_score_eval"},
        super_glue_multirc_is_a_correct_answer_: {dataset_name: "super_glue", subset_name: "multirc", template_name: "is a correct answer?"},
        super_glue_multirc_is_a_correct_answer__score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "is a correct answer?_score_eval"},
        super_glue_multirc_is_the_correct_answer_: {dataset_name: "super_glue", subset_name: "multirc", template_name: "is the correct answer"},
        super_glue_multirc_is_the_correct_answer__score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "is the correct answer_score_eval"},
        super_glue_multirc_paragraph_question_is_it_: {dataset_name: "super_glue", subset_name: "multirc", template_name: "paragraph question is it ?"},
        super_glue_multirc_paragraph_question_is_it__score_eval: {dataset_name: "super_glue", subset_name: "multirc", template_name: "paragraph question is it ?_score_eval"},
        super_glue_record_Add_sentence_after_after_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "Add sentence after after (continuation choices)"},
        super_glue_record_Add_sentence_after_after_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "Add sentence after after (continuation choices)_score_eval"},
        super_glue_record_Add_sentence_after_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "Add sentence after (continuation choices)"},
        super_glue_record_Add_sentence_after_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "Add sentence after (continuation choices)_score_eval"},
        super_glue_record_Can_you_figure_out_: {dataset_name: "super_glue", subset_name: "record", template_name: "Can you figure out"},
        super_glue_record_Can_you_figure_out__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "Can you figure out_score_eval"},
        super_glue_record_GPT_3_style_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style (continuation choices)"},
        super_glue_record_GPT_3_style_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style (continuation choices)_score_eval"},
        super_glue_record_GPT_3_style_summary_only_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style summary only (continuation choices)"},
        super_glue_record_GPT_3_style_summary_only_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style summary only (continuation choices)_score_eval"},
        super_glue_record_GPT_3_style_with_labels_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style with labels (continuation choices)"},
        super_glue_record_GPT_3_style_with_labels_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style with labels (continuation choices)_score_eval"},
        super_glue_record_GPT_3_style_with_labels_without_hyphens_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style with labels without hyphens (continuation choices)"},
        super_glue_record_GPT_3_style_with_labels_without_hyphens_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style with labels without hyphens (continuation choices)_score_eval"},
        super_glue_record_GPT_3_style_without_hyphens_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style without hyphens (continuation choices)"},
        super_glue_record_GPT_3_style_without_hyphens_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "GPT-3 style without hyphens (continuation choices)_score_eval"},
        super_glue_record_In_the_question_above_the_placeholder_stands_for: {dataset_name: "super_glue", subset_name: "record", template_name: "In the question above, the placeholder stands for"},
        super_glue_record_In_the_question_above_the_placeholder_stands_for_score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "In the question above, the placeholder stands for_score_eval"},
        super_glue_record_New_highlight_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "New highlight (continuation choices)"},
        super_glue_record_New_highlight_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "New highlight (continuation choices)_score_eval"},
        super_glue_record_News_article_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "News article (continuation choices)"},
        super_glue_record_News_article_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "News article (continuation choices)_score_eval"},
        super_glue_record_Summary_first_continuation_choices_: {dataset_name: "super_glue", subset_name: "record", template_name: "Summary first (continuation choices)"},
        super_glue_record_Summary_first_continuation_choices__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "Summary first (continuation choices)_score_eval"},
        super_glue_record_What_could_the_placeholder_be_: {dataset_name: "super_glue", subset_name: "record", template_name: "What could the placeholder be?"},
        super_glue_record_What_could_the_placeholder_be__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "What could the placeholder be?_score_eval"},
        super_glue_record_Which_one_is_the_placeholder_: {dataset_name: "super_glue", subset_name: "record", template_name: "Which one is the placeholder?"},
        super_glue_record_Which_one_is_the_placeholder__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "Which one is the placeholder?_score_eval"},
        super_glue_record_choose_between: {dataset_name: "super_glue", subset_name: "record", template_name: "choose_between"},
        super_glue_record_choose_between_score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "choose_between_score_eval"},
        super_glue_record_corrupted: {dataset_name: "super_glue", subset_name: "record", template_name: "corrupted"},
        super_glue_record_corrupted_score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "corrupted_score_eval"},
        super_glue_record_exercise: {dataset_name: "super_glue", subset_name: "record", template_name: "exercise"},
        super_glue_record_exercise_score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "exercise_score_eval"},
        super_glue_record_pick_one_option: {dataset_name: "super_glue", subset_name: "record", template_name: "pick_one_option"},
        super_glue_record_pick_one_option_score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "pick_one_option_score_eval"},
        super_glue_record_the_placeholder_refers_to_: {dataset_name: "super_glue", subset_name: "record", template_name: "the placeholder refers to"},
        super_glue_record_the_placeholder_refers_to__score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "the placeholder refers to_score_eval"},
        super_glue_record_trying_to_decide: {dataset_name: "super_glue", subset_name: "record", template_name: "trying_to_decide"},
        super_glue_record_trying_to_decide_score_eval: {dataset_name: "super_glue", subset_name: "record", template_name: "trying_to_decide_score_eval"},
        super_glue_rte_GPT_3_style: {dataset_name: "super_glue", subset_name: "rte", template_name: "GPT-3 style"},
        super_glue_rte_GPT_3_style_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "GPT-3 style_score_eval"},
        super_glue_rte_MNLI_crowdsource: {dataset_name: "super_glue", subset_name: "rte", template_name: "MNLI crowdsource"},
        super_glue_rte_MNLI_crowdsource_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "MNLI crowdsource_score_eval"},
        super_glue_rte_based_on_the_previous_passage: {dataset_name: "super_glue", subset_name: "rte", template_name: "based on the previous passage"},
        super_glue_rte_based_on_the_previous_passage_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "based on the previous passage_score_eval"},
        super_glue_rte_can_we_infer: {dataset_name: "super_glue", subset_name: "rte", template_name: "can we infer"},
        super_glue_rte_can_we_infer_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "can we infer_score_eval"},
        super_glue_rte_does_it_follow_that: {dataset_name: "super_glue", subset_name: "rte", template_name: "does it follow that"},
        super_glue_rte_does_it_follow_that_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "does it follow that_score_eval"},
        super_glue_rte_does_this_imply: {dataset_name: "super_glue", subset_name: "rte", template_name: "does this imply"},
        super_glue_rte_does_this_imply_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "does this imply_score_eval"},
        super_glue_rte_guaranteed_true: {dataset_name: "super_glue", subset_name: "rte", template_name: "guaranteed true"},
        super_glue_rte_guaranteed_true_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "guaranteed true_score_eval"},
        super_glue_rte_justified_in_saying: {dataset_name: "super_glue", subset_name: "rte", template_name: "justified in saying"},
        super_glue_rte_justified_in_saying_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "justified in saying_score_eval"},
        super_glue_rte_must_be_true: {dataset_name: "super_glue", subset_name: "rte", template_name: "must be true"},
        super_glue_rte_must_be_true_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "must be true_score_eval"},
        super_glue_rte_should_assume: {dataset_name: "super_glue", subset_name: "rte", template_name: "should assume"},
        super_glue_rte_should_assume_score_eval: {dataset_name: "super_glue", subset_name: "rte", template_name: "should assume_score_eval"},
        super_glue_wic_GPT_3_prompt: {dataset_name: "super_glue", subset_name: "wic", template_name: "GPT-3-prompt"},
        super_glue_wic_GPT_3_prompt_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "GPT-3-prompt_score_eval"},
        super_glue_wic_GPT_3_prompt_with_label: {dataset_name: "super_glue", subset_name: "wic", template_name: "GPT-3-prompt-with-label"},
        super_glue_wic_GPT_3_prompt_with_label_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "GPT-3-prompt-with-label_score_eval"},
        super_glue_wic_affirmation_true_or_false: {dataset_name: "super_glue", subset_name: "wic", template_name: "affirmation_true_or_false"},
        super_glue_wic_affirmation_true_or_false_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "affirmation_true_or_false_score_eval"},
        super_glue_wic_grammar_homework: {dataset_name: "super_glue", subset_name: "wic", template_name: "grammar_homework"},
        super_glue_wic_grammar_homework_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "grammar_homework_score_eval"},
        super_glue_wic_polysemous: {dataset_name: "super_glue", subset_name: "wic", template_name: "polysemous"},
        super_glue_wic_polysemous_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "polysemous_score_eval"},
        super_glue_wic_question_context: {dataset_name: "super_glue", subset_name: "wic", template_name: "question-context"},
        super_glue_wic_question_context_meaning: {dataset_name: "super_glue", subset_name: "wic", template_name: "question-context-meaning"},
        super_glue_wic_question_context_meaning_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "question-context-meaning_score_eval"},
        super_glue_wic_question_context_meaning_with_label: {dataset_name: "super_glue", subset_name: "wic", template_name: "question-context-meaning-with-label"},
        super_glue_wic_question_context_meaning_with_label_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "question-context-meaning-with-label_score_eval"},
        super_glue_wic_question_context_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "question-context_score_eval"},
        super_glue_wic_same_sense: {dataset_name: "super_glue", subset_name: "wic", template_name: "same_sense"},
        super_glue_wic_same_sense_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "same_sense_score_eval"},
        super_glue_wic_similar_sense: {dataset_name: "super_glue", subset_name: "wic", template_name: "similar-sense"},
        super_glue_wic_similar_sense_score_eval: {dataset_name: "super_glue", subset_name: "wic", template_name: "similar-sense_score_eval"},
        "super_glue_wsc.fixed_GPT_3_Style": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "GPT-3 Style"},
        "super_glue_wsc.fixed_GPT_3_Style_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "GPT-3 Style_score_eval"},
        "super_glue_wsc.fixed_I_think_they_mean": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "I think they mean"},
        "super_glue_wsc.fixed_I_think_they_mean_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "I think they mean_score_eval"},
        "super_glue_wsc.fixed_Who_or_what_is_are": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "Who or what is/are"},
        "super_glue_wsc.fixed_Who_or_what_is_are_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "Who or what is/are_score_eval"},
        "super_glue_wsc.fixed_by_p_they_mean": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "by p they mean"},
        "super_glue_wsc.fixed_by_p_they_mean_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "by p they mean_score_eval"},
        "super_glue_wsc.fixed_does_p_stand_for": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "does p stand for"},
        "super_glue_wsc.fixed_does_p_stand_for_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "does p stand for_score_eval"},
        "super_glue_wsc.fixed_does_the_pronoun_refer_to": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "does the pronoun refer to"},
        "super_glue_wsc.fixed_does_the_pronoun_refer_to_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "does the pronoun refer to_score_eval"},
        "super_glue_wsc.fixed_in_other_words": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "in other words"},
        "super_glue_wsc.fixed_in_other_words_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "in other words_score_eval"},
        "super_glue_wsc.fixed_p_is_are_r": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "p is/are r"},
        "super_glue_wsc.fixed_p_is_are_r_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "p is/are r_score_eval"},
        "super_glue_wsc.fixed_replaced_with": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "replaced with"},
        "super_glue_wsc.fixed_replaced_with_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "replaced with_score_eval"},
        "super_glue_wsc.fixed_the_pronoun_refers_to": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "the pronoun refers to"},
        "super_glue_wsc.fixed_the_pronoun_refers_to_score_eval": {dataset_name: "super_glue", subset_name: "wsc.fixed", template_name: "the pronoun refers to_score_eval"},
        trec_fine_grained_ABBR: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_ABBR"},
        trec_fine_grained_ABBR_context_first: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_ABBR_context_first"},
        trec_fine_grained_ABBR_context_first_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_ABBR_context_first_score_eval"},
        trec_fine_grained_ABBR_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_ABBR_score_eval"},
        trec_fine_grained_DESC: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_DESC"},
        trec_fine_grained_DESC_context_first: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_DESC_context_first"},
        trec_fine_grained_DESC_context_first_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_DESC_context_first_score_eval"},
        trec_fine_grained_DESC_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_DESC_score_eval"},
        trec_fine_grained_ENTY: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_ENTY"},
        trec_fine_grained_ENTY_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_ENTY_score_eval"},
        trec_fine_grained_HUM: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_HUM"},
        trec_fine_grained_HUM_context_first: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_HUM_context_first"},
        trec_fine_grained_HUM_context_first_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_HUM_context_first_score_eval"},
        trec_fine_grained_HUM_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_HUM_score_eval"},
        trec_fine_grained_LOC: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_LOC"},
        trec_fine_grained_LOC_context_first: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_LOC_context_first"},
        trec_fine_grained_LOC_context_first_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_LOC_context_first_score_eval"},
        trec_fine_grained_LOC_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_LOC_score_eval"},
        trec_fine_grained_NUM: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_NUM"},
        trec_fine_grained_NUM_context_first: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_NUM_context_first"},
        trec_fine_grained_NUM_context_first_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_NUM_context_first_score_eval"},
        trec_fine_grained_NUM_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_NUM_score_eval"},
        trec_fine_grained_open: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_open"},
        trec_fine_grained_open_context_first: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_open_context_first"},
        trec_fine_grained_open_context_first_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_open_context_first_score_eval"},
        trec_fine_grained_open_score_eval: {dataset_name: "trec", subset_name: null, template_name: "fine_grained_open_score_eval"},
        trec_pick_the_best_descriptor: {dataset_name: "trec", subset_name: null, template_name: "pick_the_best_descriptor"},
        trec_pick_the_best_descriptor_score_eval: {dataset_name: "trec", subset_name: null, template_name: "pick_the_best_descriptor_score_eval"},
        trec_trec1: {dataset_name: "trec", subset_name: null, template_name: "trec1"},
        trec_trec1_score_eval: {dataset_name: "trec", subset_name: null, template_name: "trec1_score_eval"},
        trec_trec2: {dataset_name: "trec", subset_name: null, template_name: "trec2"},
        trec_trec2_score_eval: {dataset_name: "trec", subset_name: null, template_name: "trec2_score_eval"},
        trec_what_category_best_describe: {dataset_name: "trec", subset_name: null, template_name: "what_category_best_describe"},
        trec_what_category_best_describe_score_eval: {dataset_name: "trec", subset_name: null, template_name: "what_category_best_describe_score_eval"},
        trec_which_category_best_describes: {dataset_name: "trec", subset_name: null, template_name: "which_category_best_describes"},
        trec_which_category_best_describes_score_eval: {dataset_name: "trec", subset_name: null, template_name: "which_category_best_describes_score_eval"},
        trivia_qa_unfiltered_first_person_context: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "first_person_context"},
        trivia_qa_unfiltered_first_person_context_score_eval: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "first_person_context_score_eval"},
        trivia_qa_unfiltered_formal_description: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "formal_description"},
        trivia_qa_unfiltered_formal_description_score_eval: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "formal_description_score_eval"},
        trivia_qa_unfiltered_guess_question: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "guess_question"},
        trivia_qa_unfiltered_guess_question_score_eval: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "guess_question_score_eval"},
        trivia_qa_unfiltered_question_answer: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "question_answer"},
        trivia_qa_unfiltered_question_answer_score_eval: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "question_answer_score_eval"},
        trivia_qa_unfiltered_question_with_instruction: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "question_with_instruction"},
        trivia_qa_unfiltered_question_with_instruction_score_eval: {dataset_name: "trivia_qa", subset_name: "unfiltered", template_name: "question_with_instruction_score_eval"},
        web_questions_get_the_answer: {dataset_name: "web_questions", subset_name: null, template_name: "get_the_answer"},
        web_questions_get_the_answer_score_eval: {dataset_name: "web_questions", subset_name: null, template_name: "get_the_answer_score_eval"},
        web_questions_potential_correct_answer: {dataset_name: "web_questions", subset_name: null, template_name: "potential-correct-answer"},
        web_questions_potential_correct_answer_score_eval: {dataset_name: "web_questions", subset_name: null, template_name: "potential-correct-answer_score_eval"},
        web_questions_question_answer: {dataset_name: "web_questions", subset_name: null, template_name: "question-answer"},
        web_questions_question_answer_score_eval: {dataset_name: "web_questions", subset_name: null, template_name: "question-answer_score_eval"},
        web_questions_short_general_knowledge_q: {dataset_name: "web_questions", subset_name: null, template_name: "short_general_knowledge_q"},
        web_questions_short_general_knowledge_q_score_eval: {dataset_name: "web_questions", subset_name: null, template_name: "short_general_knowledge_q_score_eval"},
        web_questions_whats_the_answer: {dataset_name: "web_questions", subset_name: null, template_name: "whats_the_answer"},
        web_questions_whats_the_answer_score_eval: {dataset_name: "web_questions", subset_name: null, template_name: "whats_the_answer_score_eval"},
        wiki_bio_comprehension: {dataset_name: "wiki_bio", subset_name: null, template_name: "comprehension"},
        wiki_bio_comprehension_score_eval: {dataset_name: "wiki_bio", subset_name: null, template_name: "comprehension_score_eval"},
        wiki_bio_guess_person: {dataset_name: "wiki_bio", subset_name: null, template_name: "guess_person"},
        wiki_bio_guess_person_score_eval: {dataset_name: "wiki_bio", subset_name: null, template_name: "guess_person_score_eval"},
        wiki_bio_key_content: {dataset_name: "wiki_bio", subset_name: null, template_name: "key_content"},
        wiki_bio_key_content_score_eval: {dataset_name: "wiki_bio", subset_name: null, template_name: "key_content_score_eval"},
        wiki_bio_what_content: {dataset_name: "wiki_bio", subset_name: null, template_name: "what_content"},
        wiki_bio_what_content_score_eval: {dataset_name: "wiki_bio", subset_name: null, template_name: "what_content_score_eval"},
        wiki_bio_who: {dataset_name: "wiki_bio", subset_name: null, template_name: "who"},
        wiki_bio_who_score_eval: {dataset_name: "wiki_bio", subset_name: null, template_name: "who_score_eval"},
        wiki_hop_original_choose_best_object_affirmative_1: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_affirmative_1"},
        wiki_hop_original_choose_best_object_affirmative_1_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_affirmative_1_score_eval"},
        wiki_hop_original_choose_best_object_affirmative_2: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_affirmative_2"},
        wiki_hop_original_choose_best_object_affirmative_2_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_affirmative_2_score_eval"},
        wiki_hop_original_choose_best_object_affirmative_3: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_affirmative_3"},
        wiki_hop_original_choose_best_object_affirmative_3_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_affirmative_3_score_eval"},
        wiki_hop_original_choose_best_object_interrogative_1: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_interrogative_1"},
        wiki_hop_original_choose_best_object_interrogative_1_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_interrogative_1_score_eval"},
        wiki_hop_original_choose_best_object_interrogative_2: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_interrogative_2"},
        wiki_hop_original_choose_best_object_interrogative_2_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "choose_best_object_interrogative_2_score_eval"},
        wiki_hop_original_explain_relation: {dataset_name: "wiki_hop", subset_name: "original", template_name: "explain_relation"},
        wiki_hop_original_explain_relation_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "explain_relation_score_eval"},
        wiki_hop_original_generate_object: {dataset_name: "wiki_hop", subset_name: "original", template_name: "generate_object"},
        wiki_hop_original_generate_object_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "generate_object_score_eval"},
        wiki_hop_original_generate_subject: {dataset_name: "wiki_hop", subset_name: "original", template_name: "generate_subject"},
        wiki_hop_original_generate_subject_and_object: {dataset_name: "wiki_hop", subset_name: "original", template_name: "generate_subject_and_object"},
        wiki_hop_original_generate_subject_and_object_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "generate_subject_and_object_score_eval"},
        wiki_hop_original_generate_subject_score_eval: {dataset_name: "wiki_hop", subset_name: "original", template_name: "generate_subject_score_eval"},
        wiki_qa_Decide_good_answer: {dataset_name: "wiki_qa", subset_name: null, template_name: "Decide_good_answer"},
        wiki_qa_Decide_good_answer_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Decide_good_answer_score_eval"},
        wiki_qa_Direct_Answer_to_Question: {dataset_name: "wiki_qa", subset_name: null, template_name: "Direct Answer to Question"},
        wiki_qa_Direct_Answer_to_Question_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Direct Answer to Question_score_eval"},
        wiki_qa_Generate_Question_from_Topic: {dataset_name: "wiki_qa", subset_name: null, template_name: "Generate Question from Topic"},
        wiki_qa_Generate_Question_from_Topic_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Generate Question from Topic_score_eval"},
        wiki_qa_Is_This_True_: {dataset_name: "wiki_qa", subset_name: null, template_name: "Is This True?"},
        wiki_qa_Is_This_True__score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Is This True?_score_eval"},
        wiki_qa_Jeopardy_style: {dataset_name: "wiki_qa", subset_name: null, template_name: "Jeopardy style"},
        wiki_qa_Jeopardy_style_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Jeopardy style_score_eval"},
        wiki_qa_Topic_Prediction_Answer_Only: {dataset_name: "wiki_qa", subset_name: null, template_name: "Topic Prediction - Answer Only"},
        wiki_qa_Topic_Prediction_Answer_Only_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Topic Prediction - Answer Only_score_eval"},
        wiki_qa_Topic_Prediction_Question_Only: {dataset_name: "wiki_qa", subset_name: null, template_name: "Topic Prediction - Question Only"},
        wiki_qa_Topic_Prediction_Question_Only_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Topic Prediction - Question Only_score_eval"},
        wiki_qa_Topic_Prediction_Question_and_Answer_Pair: {dataset_name: "wiki_qa", subset_name: null, template_name: "Topic Prediction - Question and Answer Pair"},
        wiki_qa_Topic_Prediction_Question_and_Answer_Pair_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "Topic Prediction - Question and Answer Pair_score_eval"},
        wiki_qa_automatic_system: {dataset_name: "wiki_qa", subset_name: null, template_name: "automatic_system"},
        wiki_qa_automatic_system_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "automatic_system_score_eval"},
        wiki_qa_exercise: {dataset_name: "wiki_qa", subset_name: null, template_name: "exercise"},
        wiki_qa_exercise_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "exercise_score_eval"},
        wiki_qa_found_on_google: {dataset_name: "wiki_qa", subset_name: null, template_name: "found_on_google"},
        wiki_qa_found_on_google_score_eval: {dataset_name: "wiki_qa", subset_name: null, template_name: "found_on_google_score_eval"},
        wino_bias_type1_anti_What_does_p_stand_for: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "What does p stand for"},
        wino_bias_type1_anti_What_does_p_stand_for_score_eval: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "What does p stand for_score_eval"},
        wino_bias_type1_anti_Who_or_what_is_are: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "Who or what is/are"},
        wino_bias_type1_anti_Who_or_what_is_are_score_eval: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "Who or what is/are_score_eval"},
        wino_bias_type1_anti_by_p_they_mean: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "by p they mean"},
        wino_bias_type1_anti_by_p_they_mean_score_eval: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "by p they mean_score_eval"},
        wino_bias_type1_anti_refers_to: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "refers_to"},
        wino_bias_type1_anti_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "refers_to_score_eval"},
        wino_bias_type1_anti_replaced_with: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "replaced with"},
        wino_bias_type1_anti_replaced_with_score_eval: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "replaced with_score_eval"},
        wino_bias_type1_anti_represent: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "represent"},
        wino_bias_type1_anti_represent_score_eval: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "represent_score_eval"},
        wino_bias_type1_anti_the_pronoun_refers_to: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "the pronoun refers to"},
        wino_bias_type1_anti_the_pronoun_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type1_anti", template_name: "the pronoun refers to_score_eval"},
        wino_bias_type1_pro_What_does_p_stand_for: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "What does p stand for"},
        wino_bias_type1_pro_What_does_p_stand_for_score_eval: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "What does p stand for_score_eval"},
        wino_bias_type1_pro_Who_or_what_is_are: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "Who or what is/are"},
        wino_bias_type1_pro_Who_or_what_is_are_score_eval: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "Who or what is/are_score_eval"},
        wino_bias_type1_pro_by_p_they_mean: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "by p they mean"},
        wino_bias_type1_pro_by_p_they_mean_score_eval: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "by p they mean_score_eval"},
        wino_bias_type1_pro_refers_to: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "refers_to"},
        wino_bias_type1_pro_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "refers_to_score_eval"},
        wino_bias_type1_pro_replaced_with: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "replaced with"},
        wino_bias_type1_pro_replaced_with_score_eval: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "replaced with_score_eval"},
        wino_bias_type1_pro_represent: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "represent"},
        wino_bias_type1_pro_represent_score_eval: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "represent_score_eval"},
        wino_bias_type1_pro_the_pronoun_refers_to: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "the pronoun refers to"},
        wino_bias_type1_pro_the_pronoun_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type1_pro", template_name: "the pronoun refers to_score_eval"},
        wino_bias_type2_anti_What_does_p_stand_for: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "What does p stand for"},
        wino_bias_type2_anti_What_does_p_stand_for_score_eval: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "What does p stand for_score_eval"},
        wino_bias_type2_anti_Who_or_what_is_are: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "Who or what is/are"},
        wino_bias_type2_anti_Who_or_what_is_are_score_eval: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "Who or what is/are_score_eval"},
        wino_bias_type2_anti_by_p_they_mean: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "by p they mean"},
        wino_bias_type2_anti_by_p_they_mean_score_eval: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "by p they mean_score_eval"},
        wino_bias_type2_anti_refers_to: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "refers_to"},
        wino_bias_type2_anti_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "refers_to_score_eval"},
        wino_bias_type2_anti_replaced_with: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "replaced with"},
        wino_bias_type2_anti_replaced_with_score_eval: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "replaced with_score_eval"},
        wino_bias_type2_anti_represent: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "represent"},
        wino_bias_type2_anti_represent_score_eval: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "represent_score_eval"},
        wino_bias_type2_anti_the_pronoun_refers_to: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "the pronoun refers to"},
        wino_bias_type2_anti_the_pronoun_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type2_anti", template_name: "the pronoun refers to_score_eval"},
        wino_bias_type2_pro_What_does_p_stand_for: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "What does p stand for"},
        wino_bias_type2_pro_What_does_p_stand_for_score_eval: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "What does p stand for_score_eval"},
        wino_bias_type2_pro_Who_or_what_is_are: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "Who or what is/are"},
        wino_bias_type2_pro_Who_or_what_is_are_score_eval: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "Who or what is/are_score_eval"},
        wino_bias_type2_pro_by_p_they_mean: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "by p they mean"},
        wino_bias_type2_pro_by_p_they_mean_score_eval: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "by p they mean_score_eval"},
        wino_bias_type2_pro_refers_to: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "refers_to"},
        wino_bias_type2_pro_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "refers_to_score_eval"},
        wino_bias_type2_pro_replaced_with: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "replaced with"},
        wino_bias_type2_pro_replaced_with_score_eval: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "replaced with_score_eval"},
        wino_bias_type2_pro_represent: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "represent"},
        wino_bias_type2_pro_represent_score_eval: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "represent_score_eval"},
        wino_bias_type2_pro_the_pronoun_refers_to: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "the pronoun refers to"},
        wino_bias_type2_pro_the_pronoun_refers_to_score_eval: {dataset_name: "wino_bias", subset_name: "type2_pro", template_name: "the pronoun refers to_score_eval"},
        winograd_wsc_wsc273_GPT_3_Style: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "GPT-3 Style"},
        winograd_wsc_wsc273_GPT_3_Style_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "GPT-3 Style_score_eval"},
        winograd_wsc_wsc273_Who_or_what_is_are: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "Who or what is/are"},
        winograd_wsc_wsc273_Who_or_what_is_are_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "Who or what is/are_score_eval"},
        winograd_wsc_wsc273_by_p_they_mean: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "by p they mean"},
        winograd_wsc_wsc273_by_p_they_mean_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "by p they mean_score_eval"},
        winograd_wsc_wsc273_does_p_stand_for: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "does p stand for"},
        winograd_wsc_wsc273_does_p_stand_for_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "does p stand for_score_eval"},
        winograd_wsc_wsc273_does_the_pronoun_refer_to: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "does the pronoun refer to"},
        winograd_wsc_wsc273_does_the_pronoun_refer_to_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "does the pronoun refer to_score_eval"},
        winograd_wsc_wsc273_p_is_are_r: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "p is/are r"},
        winograd_wsc_wsc273_p_is_are_r_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "p is/are r_score_eval"},
        winograd_wsc_wsc273_replaced_with: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "replaced with"},
        winograd_wsc_wsc273_replaced_with_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "replaced with_score_eval"},
        winograd_wsc_wsc273_the_pronoun_refers_to: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "the pronoun refers to"},
        winograd_wsc_wsc273_the_pronoun_refers_to_score_eval: {dataset_name: "winograd_wsc", subset_name: "wsc273", template_name: "the pronoun refers to_score_eval"},
        winogrande_winogrande_debiased_Replace: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "Replace"},
        winogrande_winogrande_debiased_Replace_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "Replace_score_eval"},
        winogrande_winogrande_debiased_True_or_False: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "True or False"},
        winogrande_winogrande_debiased_True_or_False_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "True or False_score_eval"},
        winogrande_winogrande_debiased_does_underscore_refer_to: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "does underscore refer to"},
        winogrande_winogrande_debiased_does_underscore_refer_to_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "does underscore refer to_score_eval"},
        winogrande_winogrande_debiased_fill_in_the_blank: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "fill in the blank"},
        winogrande_winogrande_debiased_fill_in_the_blank_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "fill in the blank_score_eval"},
        winogrande_winogrande_debiased_stand_for: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "stand for"},
        winogrande_winogrande_debiased_stand_for_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "stand for_score_eval"},
        winogrande_winogrande_debiased_underscore_refer_to: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "underscore refer to"},
        winogrande_winogrande_debiased_underscore_refer_to_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_debiased", template_name: "underscore refer to_score_eval"},
        winogrande_winogrande_xl_Replace: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "Replace"},
        winogrande_winogrande_xl_Replace_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "Replace_score_eval"},
        winogrande_winogrande_xl_True_or_False: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "True or False"},
        winogrande_winogrande_xl_True_or_False_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "True or False_score_eval"},
        winogrande_winogrande_xl_does_underscore_refer_to: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "does underscore refer to"},
        winogrande_winogrande_xl_does_underscore_refer_to_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "does underscore refer to_score_eval"},
        winogrande_winogrande_xl_fill_in_the_blank: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "fill in the blank"},
        winogrande_winogrande_xl_fill_in_the_blank_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "fill in the blank_score_eval"},
        winogrande_winogrande_xl_stand_for: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "stand for"},
        winogrande_winogrande_xl_stand_for_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "stand for_score_eval"},
        winogrande_winogrande_xl_underscore_refer_to: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "underscore refer to"},
        winogrande_winogrande_xl_underscore_refer_to_score_eval: {dataset_name: "winogrande", subset_name: "winogrande_xl", template_name: "underscore refer to_score_eval"},
        wiqa_does_the_supposed_perturbation_have_an_effect: {dataset_name: "wiqa", subset_name: null, template_name: "does_the_supposed_perturbation_have_an_effect"},
        wiqa_does_the_supposed_perturbation_have_an_effect_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "does_the_supposed_perturbation_have_an_effect_score_eval"},
        wiqa_effect_with_label_answer: {dataset_name: "wiqa", subset_name: null, template_name: "effect_with_label_answer"},
        wiqa_effect_with_label_answer_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "effect_with_label_answer_score_eval"},
        wiqa_effect_with_string_answer: {dataset_name: "wiqa", subset_name: null, template_name: "effect_with_string_answer"},
        wiqa_effect_with_string_answer_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "effect_with_string_answer_score_eval"},
        wiqa_what_is_the_final_step_of_the_following_process: {dataset_name: "wiqa", subset_name: null, template_name: "what_is_the_final_step_of_the_following_process"},
        wiqa_what_is_the_final_step_of_the_following_process_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "what_is_the_final_step_of_the_following_process_score_eval"},
        wiqa_what_is_the_missing_first_step: {dataset_name: "wiqa", subset_name: null, template_name: "what_is_the_missing_first_step"},
        wiqa_what_is_the_missing_first_step_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "what_is_the_missing_first_step_score_eval"},
        wiqa_what_might_be_the_first_step_of_the_process: {dataset_name: "wiqa", subset_name: null, template_name: "what_might_be_the_first_step_of_the_process"},
        wiqa_what_might_be_the_first_step_of_the_process_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "what_might_be_the_first_step_of_the_process_score_eval"},
        wiqa_what_might_be_the_last_step_of_the_process: {dataset_name: "wiqa", subset_name: null, template_name: "what_might_be_the_last_step_of_the_process"},
        wiqa_what_might_be_the_last_step_of_the_process_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "what_might_be_the_last_step_of_the_process_score_eval"},
        wiqa_which_of_the_following_is_the_supposed_perturbation: {dataset_name: "wiqa", subset_name: null, template_name: "which_of_the_following_is_the_supposed_perturbation"},
        wiqa_which_of_the_following_is_the_supposed_perturbation_score_eval: {dataset_name: "wiqa", subset_name: null, template_name: "which_of_the_following_is_the_supposed_perturbation_score_eval"},
        xsum_DOC_boils_down_to_simple_idea_that: {dataset_name: "xsum", subset_name: null, template_name: "DOC_boils_down_to_simple_idea_that"},
        xsum_DOC_boils_down_to_simple_idea_that_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "DOC_boils_down_to_simple_idea_that_score_eval"},
        xsum_DOC_given_above_write_one_sentence: {dataset_name: "xsum", subset_name: null, template_name: "DOC_given_above_write_one_sentence"},
        xsum_DOC_given_above_write_one_sentence_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "DOC_given_above_write_one_sentence_score_eval"},
        xsum_DOC_how_would_you_rephrase_few_words: {dataset_name: "xsum", subset_name: null, template_name: "DOC_how_would_you_rephrase_few_words"},
        xsum_DOC_how_would_you_rephrase_few_words_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "DOC_how_would_you_rephrase_few_words_score_eval"},
        xsum_DOC_tldr: {dataset_name: "xsum", subset_name: null, template_name: "DOC_tldr"},
        xsum_DOC_tldr_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "DOC_tldr_score_eval"},
        xsum_DOC_write_summary_of_above: {dataset_name: "xsum", subset_name: null, template_name: "DOC_write_summary_of_above"},
        xsum_DOC_write_summary_of_above_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "DOC_write_summary_of_above_score_eval"},
        xsum_article_DOC_summary: {dataset_name: "xsum", subset_name: null, template_name: "article_DOC_summary"},
        xsum_article_DOC_summary_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "article_DOC_summary_score_eval"},
        xsum_college_roommate_asked_DOC_so_I_recap: {dataset_name: "xsum", subset_name: null, template_name: "college_roommate_asked_DOC_so_I_recap"},
        xsum_college_roommate_asked_DOC_so_I_recap_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "college_roommate_asked_DOC_so_I_recap_score_eval"},
        xsum_read_below_DOC_write_abstract: {dataset_name: "xsum", subset_name: null, template_name: "read_below_DOC_write_abstract"},
        xsum_read_below_DOC_write_abstract_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "read_below_DOC_write_abstract_score_eval"},
        xsum_summarize_DOC: {dataset_name: "xsum", subset_name: null, template_name: "summarize_DOC"},
        xsum_summarize_DOC_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "summarize_DOC_score_eval"},
        xsum_summarize_this_DOC_summary: {dataset_name: "xsum", subset_name: null, template_name: "summarize_this_DOC_summary"},
        xsum_summarize_this_DOC_summary_score_eval: {dataset_name: "xsum", subset_name: null, template_name: "summarize_this_DOC_summary_score_eval"},
        yelp_review_full_based_on_that: {dataset_name: "yelp_review_full", subset_name: null, template_name: "based_on_that"},
        yelp_review_full_based_on_that_score_eval: {dataset_name: "yelp_review_full", subset_name: null, template_name: "based_on_that_score_eval"},
        yelp_review_full_format_rating: {dataset_name: "yelp_review_full", subset_name: null, template_name: "format_rating"},
        yelp_review_full_format_rating_score_eval: {dataset_name: "yelp_review_full", subset_name: null, template_name: "format_rating_score_eval"},
        yelp_review_full_format_score: {dataset_name: "yelp_review_full", subset_name: null, template_name: "format_score"},
        yelp_review_full_format_score_score_eval: {dataset_name: "yelp_review_full", subset_name: null, template_name: "format_score_score_eval"},
        yelp_review_full_format_star: {dataset_name: "yelp_review_full", subset_name: null, template_name: "format_star"},
        yelp_review_full_format_star_score_eval: {dataset_name: "yelp_review_full", subset_name: null, template_name: "format_star_score_eval"},
        yelp_review_full_on_a_scale: {dataset_name: "yelp_review_full", subset_name: null, template_name: "on_a_scale"},
        yelp_review_full_on_a_scale_score_eval: {dataset_name: "yelp_review_full", subset_name: null, template_name: "on_a_scale_score_eval"},
        yelp_review_full_so_i_would: {dataset_name: "yelp_review_full", subset_name: null, template_name: "so_i_would"},
        yelp_review_full_so_i_would_score_eval: {dataset_name: "yelp_review_full", subset_name: null, template_name: "so_i_would_score_eval"},
        yelp_review_full_this_place: {dataset_name: "yelp_review_full", subset_name: null, template_name: "this_place"},
        yelp_review_full_this_place_score_eval: {dataset_name: "yelp_review_full", subset_name: null, template_name: "this_place_score_eval"},
        ade_corpus_v2: {dataset_name: "ade_corpus_v2", subset_name: null, template_name: ""},
        banking_77: {dataset_name: "banking_77", subset_name: null, template_name: ""},
        neurips_impact_statement_risks: {dataset_name: "neurips_impact_statement_risks", subset_name: null, template_name: ""},
        one_stop_english: {dataset_name: "one_stop_english", subset_name: null, template_name: ""},
        overruling: {dataset_name: "overruling", subset_name: null, template_name: ""},
        semiconductor_org_types: {dataset_name: "semiconductor_org_types", subset_name: null, template_name: ""},
        systematic_review_inclusion: {dataset_name: "systematic_review_inclusion", subset_name: null, template_name: ""},
        tai_safety_research: {dataset_name: "tai_safety_research", subset_name: null, template_name: ""},
        terms_of_service: {dataset_name: "terms_of_service", subset_name: null, template_name: ""},
        tweet_eval_hate: {dataset_name: "tweet_eval_hate", subset_name: null, template_name: ""},
        twitter_complaints: {dataset_name: "twitter_complaints", subset_name: null, template_name: ""},
    }
}
