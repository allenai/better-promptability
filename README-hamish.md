# Notes on running experiments with this codebase

Note: to test on multiple dataloaders, install the latest version of pytorch-lightning (`pip install https://github.com/PyTorchLightning/pytorch-lightning/archive/master.zip`). This is because there's a metric logging fix that only got merged March 28th. **This is only needed for `configs/0shot_eval_multitask.jsonnet`.**

The generic command to run a config file is ` CKPT=null tango run -d output <config>`. Note when loading data you need to specify the mixture it comes from in the config file. `green` refers to the test-split tasks for T0 (the green list of tasks in the paper), while `d4_train` is the train set of tasks. You can see full lists in `task_lists/`. Note for the most part all tasks have train and validation splits.

You can train a model on a cluster by editing `configs/multitask_subset_specific.jsonnet` to point to the index files you want. The first file in the list will be used for the train split, and metrics will be recorded for all dev portions in the split. Note: **to train on data in the validation split, harcoded this in at line 144 of `data/data_module.py` (`dataset_dict['train'] = dataset_dict['validation']`).** Currently, the metric measured for early stopping/saving the best checkpoint is the overall average performance. The model will log performance per-cluster as specified by the index files provided at the end of each epoch, but you can easily change this in the config (note: the underlying trainer is a pytorch lightning trainer, so any normal arguments to a lightning trainer will work here).

If you just want to evaluate a model, use `configs/0shot_eval_multitask.jsonnet`. Edit the config to contain the index files you want to evaluate on. After running evaluation the model will log the per-cluster performance and the overall average across clusters (note: not weighted by cluster size).

If you want per-task performance, you can evaluate with `eval_all_clusters.sh` to evaluate on each cluster on each task. Change the checkpoint used in `configs/0shot_eval_subset.jsonnet` to change the model evaluated. The output from this can be fed into `summarise_log.py` to produce the results per-task from each cluster space separated on each line (this can be copy-pasted to google sheets easily - you can save the output doing something like `eval_all_clusters.sh &> tmp.txt`). You'll need to edit the script to point to the folder with the index files you want. Just directly run `configs/0shot_eval_subset.jsonnet` with the variables set if you just want to see performance of one model on one task in one cluster.

In case you want to generate hidden states, use `run_all_evals.sh` which just runs evals without subsets. Note that you'll have to uncomment some code in `/meta_learn_prompt/models/prefix_transformer.py` to save hidden states.
